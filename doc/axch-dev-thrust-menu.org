#+STARTUP: odd
#+STARTUP: hidestars

Venture thrust menu by axch, updated 6/15/15

composed 12/22/14; updated on 3/19/15; current on 4/16/15

Todo list convention: A '+' bullet means "done"; a "x" bullet means
"will not do (for the immediate goal)".

Dimensions on which I evaluated these projects:
- Required effort
- Does it help me feel better about Venture
- Will it help integrate Taylor
- Will it help integrate Grem
- Will it help with the summer school
  - Making Venture a system in which one would want to do the tutorial
  - Making Venture a system in which one would want to do the challenge problems
- Will it help with the papers (venturescript/metaprob)

|                            | Effort    | Me     | Taylor | Grem | SS    | Papers |
|----------------------------+-----------+--------+--------+------+-------+--------|
| Functional traces Lite     | 1-2 days  | ??     | --     | --   | ?     | ?      |
| Functional traces Puma     | ?         | ??     | --     | --   | some  | some   |
| Untraced models Lite       | 1-2 days  | ??     | --     | --   | ?     | ?      |
| Untraced models Puma       | ?         | ??     | --     | --   | some  | some   |
| First-class integers       | 1-2 days  | --     | some   | --   | some  | some   |
| More testing               | open      | ??     | some   | --   | some  | --     |
| regen SP over Lite         | drafted   | yes    | some   | --   | --    | yes ?  |
| regen SP over Puma         | 1 day?    | yes    | some   | --   | --    | ?      |
| make-sp in Lite            | < 1 day?  | ??     | ?      | --   | --    | yes    |
| make-sp in Puma            | < 1 day?  | ??     | ?      | --   | --    | ?      |
| Profiler for Lite          | 2-3 days? | yes ?  | ?      | --   | yes   | yes ?  |
| Profiler for Puma          | 2-3 days? | yes ?  | ?      | --   | yes ? | ?      |
| Reassume                   | < 1 day?  | some ? | ?      | --   | yes ? | ?      |
| Full SP serialization      | 2-3 days? | --     | some   | --   | --    | --     |
| Perf measurements          | 1-2 days  | prereq | ?      | ?    | --    | --     |
| Perf benchmarks            | 1-2 days  | maybe  | ?      | ?    | --    | --     |
| No Args in Lite            | 1-2 days  | maybe  | --     | --   | maybe | --     |
| No Args in Puma            | 1-2 days  | maybe  | --     | --   | maybe | --     |
| Family sharing in Lite     | 2-3 days  | maybe  | --     | --   | maybe | maybe  |
| Family sharing in Puma     | 3-4 days? | maybe  | --     | --   | maybe | maybe  |
| Flesh                      | open      | --     | some ? | ?    | ?     | --     |
| Bugs                       | open      | --     | some ? | ?    | ?     | --     |
| Reference documentation    | 1-2 wks   | some   | some   | some | yes   | ?      |
| Release polish             | open      | some   | some   | some | yes   | ?      |
| Refactorings               | weeks     | --     | yes ?  | --   | --    | --     |
| Hs-RandomDB-v1             | 1 wk?     | yes ?  | ?      | --   | --    | maybe  |
|----------------------------+-----------+--------+--------+------+-------+--------|
| Web demos on HsVenture     |           |        |        |      |       |        |
| Integrate blog demos       |           |        |        |      |       |        |
| Multi-makers               |           |        |        |      |       |        |
| Mixmh combinator           |           |        |        |      |       |        |
|----------------------------+-----------+--------+--------+------+-------+--------|

*** General areas of responsibility for the Venture organization (notes circa mid-late 2014)
- Professional development of members
  - Dissertations, visible projects
- Conducting interesting research (people understanding things,
  implementing ideas, experimenting)
- Output of interesting research (papers, talks, demos)
- Fulfilment of contractual obligations
  - Solutions to the challenge problems
  - Presentations at PI meetings and calls
  - Obligations vis a vis other funders
- Output of grant proposals or pitches, and content therefor.
- Output of interesting demos (web, IPython, maybe others?)
- Software quality (absence of bugs, incompletions, surprises)
  - Does this mean simplicity of a decent mental model of what will or won't work?
  - Sub-area: thoroughness and robustness of software testing
    - correctness, invariants, statistical, liveness of artifacts
  - Sub-area: coherent language and library design
- Richness of software features
  - One aspect is ease of access for standard techniques and obvious
    combinations of them
  - Another is ease of implementation of research experiments
- Software performace (of user experiences with all artifacts)
  - First order of business: end-to-end measurements
  - The weight of network traffic may be a bottleneck
  - The stack may be a bottleneck
- Effectiveness of development environment(s)
  - as a means to all the other ends
- Output of usable, installable artifacts
  - With good documentation, tutorials, examples, teaching material
- Output of interesting ideas (blog posts)
*** Project: Functional traces
- Benefits:
  - Make (sophisticated) particle methods have better performance
  - Make dynamic programs representing multiple distributions not
    stupidly inefficient (e.g., forward-backward algorithm, or
    inside-outside for pcfgs)
  - Nested forking
  - Top-level pgibbs as an inference program
- LKernel cleanup would make this somewhat easier
  - could cut LKernels if needed
  - could replace AAALKernel with another special SP thing, parallel
    to density (logDensityOfData)
- Could be implemented by reproducing the trace interface from Lite
  - regen and detach might be sharable, if they can be written in a
    functionality-agnostic style (which might actually be somewhat
    difficult)
    - particles as practiced are not functional traces -- they're
      imperative traces with functionally shared parts
    - that might actually be ok for "functional traces" too -- an
      imperative shell around functionally sharable parts
  - do particles currently have a bug with undercloning shared auxes?
- Hack: leave the interface to a trace itself imperative, but confine
  the mutation to the trace's direct field pointers, and have all
  those be persistent data structures.  Then "copy" is still an
  operation, but now O(1).
  - Is it appropriate to make a version of detach that does not build
    a rhodb, or do we want that anyway?
    - Actually, this feels orthogonal, because there may be situations
      even with imperative traces where rhodb is not needed
  - Is this what a "particle" is?  Can I just make "particles" without
    a base trace?
  - Still room to munge the inference primitives to take maximal
    advantage
*** Project: Untraced models
Notes on the untraced backend
+ mkdir backend/untraced
+ design desideratum: exactly reuse the Lite SP library
+ still want to support addresses (e.g. for error reporting) so will
  need to carry them through the interpreter (and catch and annotate
  errors in the same way, etc).
+ will probably want to make bogus nodes for requesters that just
  contain their values and nothing else
+ will need to call incorporate like the PET does
- question: do I try to respect on-SP LKernels?
- scopes and blocks probably don't make much sense anymore, so can't
  really do much block-controlled stuff

Outstanding work:
+ I worry that my shoot-from-the-hip evaluator may not transmit auxes
  properly.  Are there tests for auxes in the inference trace?  Should
  I attempt to use the untraced backend for the model trace?
+ I worry that ignoring LSRs may not be the right thing, even when
  untraced
+ mem is probably broken in the untraced interpreter, b/c I did not
  implement the request id protocol
+ Since there is no unevaluation, will leak entries in request-result
  maps.
  + One mitigation strategy: make an explicit marker for a "do not
    track" request id and do not track them
- Test case: run the sp property tests (then maybe all the conformance
  tests?)
  - First, define a backend named Untraced
  - Likely to break all the collapsed samplers, if they are tested at
    all
x Should also resurrect core-slam.vnt and run it with untraced
  inference (watch performance)

Choice: do I implement the interface that engine.Trace expects, or the
one it provides?  Or do I make engine.Trace a true combinator?
- Making the choice commits me to either storing or not storing the
  source code and value history of toplevel directives
- Decision: implement the interface that engine.Trace expects.  Maybe
  it will become a true combinator later.

Comment: The untraced interpreter is the list of all the extra noise
that Venture creates that is not actually used to track dependencies.

Later:
- can support a somewhat limited form of OBSERVE via an
  eval-constrained method, which
  - does whatever constrain does on constants (crash or equality test)
  - crashes on lookups [*]
  - on applications
    - evaluates the arguments unconstrained
    - runs the requester if any unconstrained
    - if the outputPSP is ESRRefOutputPSP, evals the first request
      constrained (and the others unconstrained)
    - else, evals all the requests unconstrained and calls the
      logDensity of the outputPSP, returning the weight
- Possible UI hack: have OBSERVE do nothing, but set a flag disabling
  further non-OBSERVE operations (including resample!) until an
  INCORPORATE.
- The above should suffice to implement concurrent-particle likelihood
  weighting and particle filtering; and also rejection sampling if
  density bounds are available (can be extracted by eval-constrained
  as well).
  - Storing the program text enables stream likelihood weighting
    - Maybe the inference program's source code is a sufficient store
      of the program text, where one (in_model ...) call suffices for
      one particle/likelihood trial.
- Actually, it seems as though the program text is sufficient to
  implement global regeneration over an untrace (with only the top
  random choices of the directives for principal nodes).  Does this
  mean I can do (resimulation_mh default all ...)?  Or is detach (and
  unincorporate) a problem?

[*] Crashing on lookups means can't
  [assume x (stuff)]
  [observe x 5]
This program is equivalent to
  [observe (stuff) 5]
  [assume x 5]
Observing variables in general requires PETs, because need to
repropagate the new value (which actually makes problems even for
PETs).
*** Project: First-class integers
- Check that Puma has an integer type
- Decide what the density of a continuous-valued SP should be on an
  integer output.  -inf, or convert the integer to a float?
  - If the former, do I want to flag that situation, to detect the
    tons of soon-to-be-impossible conditions pervading our test suite?
- Teach the parser to produce integers (test on a constant)
- Introduce a NumberOrInteger type, and make Lite arithmetic generic over floats and ints
  - Test on some trivial examples, and with the existing randomized tests
- Devise (abstract?) some boilerplate for genericity in Puma and make
  Puma arithmetic generic
- Go through the types of all the builtins and make integers where appropriate
*** Project: make-sp
- Define a class named something like SyntheticSP, whose methods etc.
*** Project: A normal profiler (based on addresses)
- specific suggestion: get profiling data on SLAM
  - problem: the profile data is almost certainly not serialized or
    deserialized, so resampling would tend to lose it
  - problem: there is some directive id mismatch bug in the ripl's
    post-processing of the profiler data involving sivm resugaring
    - could gain more insight into it by making the sivm assign
      directive ids
- Milestone: When we have shown using the profiler that it is faster
  to write an SP in Python
*** Project: Redefine/reassume as uneval, eval, rebind, propagate
the latter being what incorporate does
*** Project: Full SP serialization
- I keep thinking that I can avoid having to explicitly serialize
  primitive and compound SPs, because I should be able to serialize
  just the random content, and then rerun the maker choice with the
  same random content to restore.
  - The interface adjustment would be to values: "tell me your
    serializable random content" and "update your random content with
    this deserialized thing".
- Problem with this plan: categorical.
  - In effect, categorical has latent stochastic control flow, in that
    it can return closures with different bodies depending on which
    way the internal flip goes.
  - The "random content" of a categorical flip is the index of which
    of its arguments it chose last time.
- Could add yet two more methods to the SP interface:
    psp.reconstructionInfo(value, args) -> VentureValue (presumably serializable)
    psp.reconstruct(info, args) -> VentureValue
  Categorical would return the atom in the first case, and that answer
  in the second.
  - The serialization mechanism would wrap reconstruction info in an
    extra tag telling the deserialization mechanism to use the
    reconstruction code path rather than just replacing the value.
  - Still problematic, because categorical would need equality on
    proedures to answer this question (but, of course, it still needs
    it in order to absorb).
- May be able to fix the categorical problem by serializing SPRefs
  using stable addresses, and only doing something interesting when
  the SPRef points to the node it is in.

Associated bug (circumstances of discovery unknown): Random variables
of type SP break the second resample_multiprocess
*** Project: Collect a suite of performance test problems
Only requirement: we abstractly want to make them faster
- Challenge problems
- Examples (including lda, crosscat, curve fitting with pygame)
- Web demos

Set up push-button profiling (and time measurement)
- cProfile for Python stuff (can I get a Venture commandline argument
  to profile itself?)
- startprof also an option for Python stuff
- what for Puma?

Summary of extant results, as given to Dan Roy on Flowdock on May 13,
2016:
- For [PPAML] CP5, we bound a third-party parser and chewed up one
  section of the Penn Treebank in reasonable time (I forget whether it
  was hours or minutes).
- For CP1 we did SLAM on 90 seconds of laser data (25 frames per
  second, I assume).
- CP 4.1 was a 5-D 500-pt Bayes linear regression, and I was able to
  draw enough samples on my laptop to convince myself that the program
  worked.
- One of our things for CP4.3 was a 10,000-wide particle filter on a
  20-step discrete HMM that ran in a couple hours.
- I also hear rumors of someone getting an LDA to run on a 1M-token
  corpus, but I haven't seen that reproduced.
*** Project: Start a suite of micro-benchmarks (ideally with baselines)
Specific micro-benchmarks:
+ long simple Markov chain on simple model (normal-normal) in Puma.
  + resimulation MH stresses just detach and regen
  + slice also checks slice logic
  + pgibbs stresses particles, pgibbs logic
  - egibbs on coin flips
+ long simple Markov chain on simple model (normal-normal) in Lite
  + resimulation MH
  + slice
  + pgibbs
  x nesterov
  + HMC
  + rejection
  - egibbs on coin flips
- big likelihood weighting run on simple model in Puma
- big likelihood weighting run on simple model in Lite
  - Is rejection sampling the same thing?
- long simple Markov chain on more complicated model in Puma/Lite.  Possible
  issues (both for Venture and for the comparative baseline):
  - selection of subproblems
  - creation/destruction of brush
- SMC or particle filter on simple series model in Puma/Lite.  Stresses:
  - resampling
  - inference program interpretation, somewhat
- Same thing multiprocess.  Stresses distributed resampling.
- long complicated Markov chain (with many operators) as a tall
  inference program repeat.  Stresses:
  - inference interpretation
  - crossing whatever barriers
- Same thing multiprocess (parallel chains).  Stresses lockstep execution.
- long complicated Markov chain as a Python program.  Stresses:
  - jumping in and out of inference program interpretation; parsing

Can use any surprises from profiling the test corpus for more inspiration.
*** Project: Simplicity and performance: Flush the Args struct
in a way that simplifies the SP interface

Some (much?) of the performance gain here has been gained by replacing
the Args fields with methods, such that they are not computed unless
called for.
- Could still memoize the methods, if desired
*** Project: Performance: Share static dependency info across instances of the same family
Allocate the static portion of a family's dependency structure only
once, and have each node contain a reference and an index to it.
- This shares the memory for representing (static) dependencies

After that, could concievably share scaffold construction, or even
precompiled detachments or regenerations.
*** Project: Performance: Fast-path NullRequestPSP and ESROutputPSP
*** Project: Finish the Foreign SP Author's guide (notes from 4/20/15)
- Note: cleaning up LKernels would simplify the foreign interface
  (somewhat)
  - Actually, one option is to leave LKernels as they are,
    representing proposals that have cancellations against the prior,
    and introduce another object that doesn't, for, e.g., Gaussian
    drift (and, of course, the DeterministicLKernel)
    - Do we need accommodations for such things, or can they be
      handled entirely in the inference program?

Outline:
Venture Foreign SPs
- What is a Venture stochastic procedure?
- When should I write a foreign SP for Venture?
- How do I write a foreign SP for Venture?
  - Just functions
  - Distributions with densities
    - Absorbing at some arguments but not others
  - Gradient methods 1: density gradients
  - Gradient methods 2: simulation gradients
    - Randomness control
  - Rejection: density bounds
  - Enumeration
  - Distributions with sufficient statistics
    - Just sufficient statistics [We don't actually have any of these
      in the standard library --Ed]
    - Gibbs proposals
    - Collapsed models
  [The rest of the interface is about LKernels, which have essentially
  bit-rotted for me --Ed]
  [And then there are latents a la lazy foreign hmm.  Would need to
  reconstruct that --Ed]
- How do I write a foreign SP for the Puma backend in C++?
  [Short answer: Don't. --Ed]
  [Longer answer: The example is in cp4/p1_regression/ --Ed]

What parts of the system do I want to show autogenerated documentation
for, and to whom and for what?
- the shortcuts module is the entry point to the programmatic api
- the Ripl class is the center of the programmatic api; also
  used by plugins
- callbacks get an Inferrer
  - which gives them access to an Engine (but maybe I want to hide that)
  - and a Ripl
- the Types are necessary for annotating foreign SPs
- there are a bunch of combinators for convenient SP definition
  (currently in builtin.py, but probably shouldn't be there anymore)
- there are a bunch of base classes for somewhat less convenient SP
  definition (psp.py)
- the actual SPs and Auxes may be needed for foreigns with nontrivial
  state and incorporation (sp.py, not counting SPType; but SPFamilies
  and SPRecord are completely internal)

The convenience combinators that make sps (suitable for
ripl.bind_foreign_sp) should live in the sp module.  There, less
important, one can also make a custom SP class by inheritance, or make
a regular SP out of custom PSPs.
- There should be one combinator for functions
  - description should be optional
  - gradient of simulate should be optional
  - can the type be optional?
- Combinator could take the requester as an optional argument, or I
  could define one with a different name that expects the requester.
- Could keep the ones in builtin.py for convenience for now, and
  migrate the codebase off them later.
  - Don't even need reuse, I suppose, except to make sure testing works.
  - builtin imports sp anyway.
  - psp imports lkernel which imports sp (but only for VentureSPRecord)

Map of information provided to methods that need to be implemented, or
subclasses that need to be derived from:
- simulate
- gradientOfSimulate
- isRandom tells whether simulate is actually stochastic, making it
  a valid or invalid principal node
  - Derive subclass?
- canAbsorb goes together with logDensity, and describes circumstances
  when an SP claims to be happy on the absorbing border.
  - Unlike what it says, -inf logDensity is occasionally ok; will
    just reject the transition.
- logDensity
- gradientOfLogDensity
- logDensityBound
- logDensityOfData
- madeSpLogDensityOfDataBound
- incorporate
- unincorporate
- canEnumerate goes with enumerateValues
- enumerateValues
- description and description_rst_format is for autogenerated
  documentation, only relevant for builtins.
- the rest are basically bit-rotten; they were about changing the
  default proposal distribution
  - and I never understood what hasVariationalLKernel was all about

Other nitpicks:
- SPFamilies is just renaming some dict methods (in Lite).  Why do
  I need it?
- Get rid of the wildcard import of types.py in value.py

Idea: define a surface syntax for Venture type annotations
- Taylor recommended sticking with combinators for now
- could move them to a separate module and remove "Type" from the name
- could also define pre-instantiated versions of the parameterless
  ones with lowercase names
*** Project: Perfect the web demos running on HsVenture (forget, memory leak, inference quality, cleanup)
What would it take to run the curve-fitting demo?
- Stretch win condition: a fast backend that can do gradients!
+ Step 1: log all requests and responses server-side, to be able to debug
x Step 2: check out Baxter's suggested ghc-mod for in-editor type checking
  - Could do a grammar pass on the documentation thereof via github
  - To make this work, I would want to either upgrade to GHC 7.10+ or
    downgrade Cabal to before 1.22
    - The error is
      Fail errors:

      BUG: /home/axch/work/pcp/Venturecxx/backend/hs/dist/setup-config: hGetContents: invalid argument (invalid byte sequence)
  - Query out to Baxter, 4/28/15
+ Get the server to talk crossdomain mumbo-jumbo properly
+ Split off from Server a WireProtocol module that exports a function
  run :: (Command num) -> IO (Either String ByteString)
  + start with no either; encode errors later
  + generalize to unknown directive type
- Interpret all requests the demo makes
  + list directives
    + record the directives on the Model
    + pretty-print them
    + in the proper format
    - refactoring: can use .= to make Pair objects, or not
    - future bug: quote literal lists where appropriate when rendering an expression
  - stop continuous inference
    - can hold the thread id in an IORef, and have stop
      grab the model mvar and then send a thread kill with killThread
      - actually, warp might run the application multithreaded, so
        another MVar might be better.
  + clear
  + set_mode (ha!)
  + assume should already work
  + how much support do I need for labeled assume, observe, predict?
    - the client relies on labels being echoed back to extract data
      points from them
      - why use a labeled predict to store a piece of state on the
        model instead of an assume?  You're programmatically
        synthesizing the name anyway...
      - historical advantage: one used to not be able to forget assumes
      - probably can't get away from the labeled observe anyway
        (except by introspecting on the expression?)
    + could do it by maintaining a bidirectional map between labels
      and addresses in the same MVar as the model (due to intertwined
      invariants).
  + observe
    - might be nice to define a separate entry point into the parser
      for the values
  + predict
  + infer
  + infer loop
    x tune the number of transitions it takes for good performance?
  - forget
    + in the demo as written, forget relies on the server echoing
      integer directive ids, too (absent which, sends a "null" as the
      directive id to forget!)
      + use the integer value of the Addresses as the ids
    - remove the directive from the directive map
    - if it was an observation, unconstrain
      - unconstrain is a problem because I need to know when to stop,
        and which node to add to the randoms set.
    - uneval the root expression
      - uneval is a problem, because it entails reference counting or
        garbage collection, and I don't have it yet.
    - if it was an assume, unbind the symbol
    - note: unlike a Trace, a Model is a complete object.  It admits a
      notion of garbage collection, and of checking the random choices
      set.
- Model SPs should be easy (deterministic ones should be very easy)
  + But, need to add "quote"
  + true, false
  + I seem to be lacking deterministic + (and who knows what else)
  + uniform_continuous, flip for inferring outliers
  + sqrt, inv_gamma for inferring noise
  + tag, uniform_discrete, maybe parsing >= for the advanced model
  + variadic + (and maybe *) for the advanced model
  + gamma, make_crp for the clustering demo
    - might want an optional d parameter for the crp
  - I should add unit tests for uniform_continuous, sqrt(?)
  - I should probably do a quality test involving the {inv_}gamma
    distributions, to make sure I haven't made any strange mistakes.
  - I should probably do quality tests for CRP to make sure I got it right
- The advanced model of the curve fitting demo is leaking memory.
  Looks like the trace accumulates garbage, because clearing reduces
  memory use.
- The clustering demo looks visually terrible -- how should I debug
  its inference quality?
  - Issue: proposals involving changing the CRP alpha will rebuild the
    entire process.  Where are these absorbed?  Do they end up
    destroying and resampling the cluster parameters?  If so, why are
    they accepted so often?  Or are they?
  - Debugging strategy:
    - Confirm correctness of simulators and densities of gamma,
      inv_gamma, uniforms (by statistical tests)
    - Introduce Integer type to avoid possible screw-ups with floating
      point stuff (also use for uniform_discrete)
    - Confirm correctness of crp in isolation (how, exactly?)
    - Teach make_crp to absorb changes to the parameter (how do I do
      that? ReferringSPMaker?)
+ I need to deal with if
+ Test that restarting the client doesn't clobber the server
+ Test that changing the model works
- I have a problem with out of order definitions, because my Envs are
  not recursive :(
- I also have a problem with queueing requests client-side, because (I
  think) the "done" callback is not invoked until the queue empties,
  which is not the right thing at all for streaming list_directives.
  - Not sure that's true; the observed slowness may just be due to the
    Firefox debugger having high overhead (does logging request bodies
    matter here?)
+ When I get to benchmarking, the path can be
  - Make a commandline program that accepts a transition count and
    runs a tiny model for that many steps of MH.
  - Profile that and improve things until it stabilizes
  - See whether the server still exihibits any interesting performance
    issues
+ Later, I will want to either generalize the Haskell parser to accept
  json numbers and booleans here and there, or adjust all the other
  demos to send strings everywhere.
- Problem for later: I want derivatives to be able to travel through a
  CRP log density to its alpha parameter if they need to, but I also
  want to permit lifting a non-differentiated CRP alpha into a
  derivative that is proceeding without it.  These two desiderata
  create a problem for the type signature of crp_log_d.
  - Also, this sounds like I am back to needing SPs that can be
    fmapped to change their stored number type.
    - This is, however, not the same as the problem I had before.  Now
      it's just about mapping the aux if it has relevant numbers in
      it.
  - Does Lite do this right?  Propagating derivative information
    through the aux of a CRP?
- Later, I may want to do a dead code elimination pass on jripl.js
- Later, I need another intermediate language, corresponding to the
  interior of quote.
  - parse :: String -> Intermediate
  - expand :: Intermediate -> Exp (with combinators like v_if expanded)
    - quote produces literal values
    - theoretically I have a choice of what value quote produces;
      e.g. I could use exp_to_value on the final results.
      However, it seems more sensible to let the Intermediate type be
      Value
- Later, I will want to either include the ExamplesEmbedded in the
  test suite or flush them
- Later, I will want to port other demos to HsVenture
- Later, I may want to test that changing clients works (that is,
  swapping to a different demo)
- Later, could contribute to Data.Bimap by expanding the interface to
  look like Data.Map.
  - fork, pull, code, push, send pull request
  - the real story would be type-level selection of representations in
    both directions, which seems to call for a mapping typeclass.
- Later, could edit to documentation of Data.CircularList (if I care),
  or ghc-mod for grammar.

----------------------------------------
Profiling notes.
- It leaks, of course.
- had to blow away my sandbox and rebuild with library profiling on
  (but actually that wasn't too bad)

Process:
- cabal configure --enable-profiling
- cabal build benchmark
- time dist/build/benchmark/benchmark 10000 +RTS -hy && make benchmark.pdf
- evince benchmark.pdf

compilation notes: cabal test, cabal build venture-server, cabal build benchmark

Initial state:
- 1.8 seconds (profiled) for 10000 steps on the observed normal-normal
  model building heap at a rate of about 1MB/sec (1400k for the run)
- 11s and 450k if SP.current is marked strict
- same pattern as strict SP.current but faster on cbeta-bernoulli; not
  affected by removing the strictness annotation.
  - scaling is worse than linear.  Why?
- time venture puma -e '[infer (do (assume x (normal 0 1)) (assume y (normal x 1)) (observe y 2) (incorporate) (resimulation_mh default one 100000))]'
  takes 0.5 seconds to start up
  after that, 100,000 transitions in 3 seconds
- Lite, 10,000 in 8
- time venture puma -e '[infer (do (assume coin (make_beta_bernoulli 1 1)) (assume f (coin)) (resimulation_mh default one 200000))]'
  100,000 transitions per second
- Lite, a little under 10,000 transitions per second
- A little C program for the normal-normal chain does 10,000,000
  transitions in 1.7s -- 200x better than Puma

The first duplex of problems was a thunk leak for states of SPs that
have no state (and thus do not read it), and a GHC bug:
https://ghc.haskell.org/trac/ghc/ticket/10359

After fixing that, 8 seconds profiled for 50,000 steps of
normal-normal, 4 for 50,000 of cbeta-bernoulli
- unprofiled, 6.6s for 100,000 steps of normal-normal
- 3.1s for 100,000 steps of cbeta-bernoulli

Residual laziness:
- Function arguments, etc.
- I don't know whether Data.Sequence.Sequence is strict or lazy in the
  elements
  - Stack overflow "Is there a stricter Sequence?" seems to think
    Sequences are element-strict but spine-lazy.
  - The documentation also says "strict operations"
  - Experimentally, sequences are not element-strict
- I am reasonably confident that my InsertionOrderedSet is
  element-strict, because the elements are used as keys in a map.
+ SPs are lazy in the state
+ The actual state of make_cbeta_bernoulli and mem might have laziness
- mem tables might still be key-lazy, though I doubt it
? The maps in Trace and SPRecord are handled lazily
  - Lenses I use, e.g. ix, might be lazy in e.g. map values
    - In particular, ix falls back on lazy insert
x May wish to fold NFData into Numerical

----------------------------------------
Other notes:
- The win condition for most of these cleanups is "I look at the
  relevant piece of code and it doesn't look ugly to me".
- I probably want variable names to be my own type, rather than Text
  - map (DT.pack . show) $ ([1..] :: [Int]) is a pretty dumb way to get
    a bunch of unique variables.
+ Might not want to store a Bimap to Strings in the server state
  - Might also want a strict version...
- Might want to rename the imports of strict Maybe to something smaller
  and less obvious after I have flushed lazy Maybe
  - May need to hide Prelude stuff
  - SP.hs
  - Trace.hs
  - Regen.hs as a matter of convention
x Might be nice to replace addFreshNode with a device for making a
  request-output node pair together, to simplify the types.
- Choice: should responsesAt lens to a list or a vector?
- Choice: Do I want the haskell functions that implement parts of an
  SP to take lists or vectors of e.g. values?
- Might be a good idea to migrate the current state field of an SP to
  SPRecord instead, to avoid copying the other 8 fields of SP every
  time it changes.  This is mildly a pain because it will force the
  existential types to move around.
- Do I even want the node graph to be fully strict?  That may weaken
  the asymptotics of gradients.
  - What alternative do I have?  Strictness annotations on all
    functions that manipulate these things?  What discipline can I
    follow?  How does Data.Map.Strict do it?
  - Should I just upgrade to GHC 7.10 and make the whole thing strict
    by default?
- Making SPs lazy in the state remains tempting, because of a history
  of work saved for stateless SPs (at the cost of a thunk leak).
  - Do I want to implement incorporation avoidance for stateless SPs
    expliclty?
  - Does this matter anywhere near as much now, given how much cheaper
    incorporation got?
- On typeclasses for SP state operations:
  - It is tempting because it will simplify the code and reduce the
    quantity of boilerplate functions.
  - May also improve performance by reducing copying of SP records,
    and possibly simplify migration of the state to SPRecord instead.
  - The actual typeclass story is:
    - an AbelianGroup a  (is there a library definition of this class?)
    - a state type s with an AbelianGroupAction a s (is there a
      library definition of this? If not, how should I encode it?)
    - a homomorphism from Value num (to be incorporated) to a
    - a homomorphism from the request nonsense to a
    - this works great for () state and for cbeta-bernoulli state
    - looks kinda clumsy for the state of mem (a pair of an insertion
      set and a deletion set? group operations by set union and set
      difference? I guess...)
    - is tempting to simplify to a and s being the same with the
      standard self-action, but doesn't capture all the flexibility of
      the current regime
+ Do I want to abstract non-requesting SPs (there are plenty of them!)
  - Issue: technically, declaring a lack of state and a lack of
    requests should commute, but it's not obvious how to do that.
  - Alternately, I may want to move to the trampoline style completely.

- Should test the mem implementation (and keep an eye on how well AD
  handles it)
- I hacked around the fact that the inverse beta function is defined
  only on Doubles by dropping gradients around it, which is poor.
*** Project: Integrate Wadden's Blog demos
*** Project: Multi-procedure makers (by true downstream abosrbing? by true multivalue returns?)
Good thing to do: change makers to say "I am in charge of everything
that happens to my output value, be it a single SP or not".
- bug, encountered by Zinberg: deterministic consequences still need
  to be propagated (well enough).  e.g., if claiming AAA of a list of
  SPs that may close over state (e.g. from the parameters to the
  maker), need to propagate that state to locations that extract
  values from that list as inference proceeds.
- does Church-encoding the list solve this problem?
  - I would tend to assume not
- another possible approach: cause the made SP to be responsible for
  its own applications
  - problem: what if it's taken out of the list multiple times?
- another possible approach: make the list contain nodes, or perhaps
  implicit nodes
- might also be fixable with true pattern matching and multivalue
  returns
  - second-class multivalue returns a la Scheme are actually
    appropriate for a "machine language"
*** Project: Mixmh combinator in the inference programming language
Should be able to make mixmh be a combinator (not necessarily with
that name).
- Takes an assessable function from the current state to something
- Makes an auxiliary variable out of that
- Knows how to complete a weighted proposal that reads this variable
  to one that includes it (thus chainable)

Two analyses of a Markov chain with state X, auxiliary variable given
by p(v|x), and conditional proposal q(x'|x,v):
- Persistent augmentation:
  - Expand the state space to X x V
  - One move is to resample v by p(v|x); this is a Gibbs step on v
  - Another move is to propose (x',v) where x' ~ q(x'|x,v).  The
    acceptance ratio is
      p(x') p(v|x') q(x|x',v)
      -----------------------
      p(x)  p(v|x)  q(x'|x,v)
    which evidences the correction p(v|x')/p(v|x) to the MH ratio as
    it would obtain for moving on x alone, or if v were independent of
    x.
- Transient augmentation 1:
  - If we rigidly cycle between moves on x and moves on v, it is not
    necessary to store v between them, so the same analysis justifies
    the same acceptance ratio for a move q' on x consisting of
      v  ~ p(v|x)
      x' ~ q(x'|x,v)
- Transient augmentation 2:
  - If we can integrate v out of the above proposal, however, we can
    have an acceptance ratio of
      p(x') q'(x|x')
      -------------
      p(x)  q'(x'|x)
    where q'(x'|x) = sum_v q(x'|x,v) p(v|x)
- Blend:
  - If v can be factored into an assessable component v1 ~ p(v_1|x)
    and a component v2 such that q(x'|v_2,v_1,x) p(v_2|v_1,x) is
    marginalizable over v_2, those two can be analyzed in those two
    ways.

Question: Is integrating v always better?

Relationship: Transient 1 can be read as using stochastic one-point
estimates of the integral involved in Transient 2, with the proviso
that it be the same point in both places.
- Intuitively, one should be able to use a k-point estimate of the
  integral.
- What if I propose like this:
  - {v_i} ~ iid p(v|x)
  - i     ~ uniform 0 n
  - x'    ~ q(x'|x,v_i)
- Then I assess auxiliarizing {v_i} and integrating i:
    p(x') p({v_i}|x') q'(x|x',{v_i})
    -------------------------------- 
    p(x)  p({v_i}|x)  q'(x'|x,{v_i})
  Where q'(x'|x,{v_i}) = (1/n) sum_i q(x'|x,v_i)
- This is not actually a k-point estimate of the integral.
- If the v_i are independent of x, this assessment does form a k-point
  estimate of the integral of q(x|x',v) wrt v.

Question: Is there an algorithm and analysis that leads to the
acceptance ratio
  p(x') q'(x|x',{v_i})
  --------------------
  p(x)  q'(x'|x,{v_i})
where
  q'(x'|x,{v_i}) = sum_{v_i} p(v_i|x) q(x'|x,v_i)

Partial Answer: Choosing i weighted according to p(v_i|x) will produce
that term in the acceptance ratio, but will not eliminate the
prod_{v_i} p(v_i|x) term.
*** Project: Develop intro Venture curriculum for new team members (notes circa mid-late 2014)
Item: evaluation model of the modeling language
- always in the stochasticity monad
- always have a sample
- Ben got a little stuck on this, thinking that x named the random
  variable

The vision is a mini course on aspects of Venture that ideally would
be taught to new team members.

Motivation
- Operating with uncertain knowledge, blah blah blah
- "A probabilistic program is a representation of a probability distribution"
  - Representations include samplers, density/measure "assessors", maybe importanters
  - Assessors are bad at marginalization, normalization, integration
  - Samplers are bad at conditioning, tolerable at integration
Feasibility of and assumptions for Metropolis-Hastings
- Goal is conditioning, which is tough
- Markov chains
- Detailed balance
  - Nice that you only need density ratios
- Acceptance ratios
Foundations/terms (measure theory angle)? [See "After talking with Ken Shan"]
Traces, Scaffolds, (Detach), Regen -- basics of local inference
- The fully general Gibbs idea is that approximately sampling from the
  local posterior helps with the global posterior.
- brush
- mixMH?
- are requests here?
The SP interface
- requester, simulate, log density, gradients, etc
Exchangeable coupling, AAA, collapsed and uncollapsed models
- Venture has uncollapsed conjugate models that take Gibbs steps (?)
- Venture could also have uncollapsed non-conjugate models that abosrb efficiently
Reverse-mode AD, in Venture
- Draw the pictures by analogy to forward mode
- The trace is the tape; detach is the reverse phase
Inference strategies that are common enough for Venture to implement, and their relationships
- Default MH, Rejection, (e)gibbs, pgibbs, slice, meanfield?, emap, nesterov, hmc, SMC via resample
The fixing randomness trick for coherent stochastic estimation of the local posterior
- for HMC, gradient ascent, slice, maybe egibbs
Other bonus features
- LKernels
- Latents
- AEKernels
Surfaces
- console, Python library, IPython notebook, server, web terminal?,
  peek/plotf, Analytics, VentureUnit
Testing strategy(ies), code style, repository structure, team tools, broad code map
*** Project: Design a good experiment runner (independent of Venture?) (notes circa mid-late 2014)
VKM on the needs of computational science:

--- interactive acquisition, analysis and visualization of the results
from computational experiments

--- interactive development of such experiments, including iterative
parameter adjustment, increases in resolution, plot changes, ...:
- 100 expts, .1s each: interactive w/ single threaded program
- 1000 expts, .1s each, or 100 expts, 1s each: interactive w/
multiprocessing on a 64 core machine
- 1000 expts, 1min each: multiple cycles per day on 64-core machine
w/ multiprocessing
- 1M expts, .1s each: should be single threaded again
  - or batched, presumably
- later on, larger scales could use starcluster/ipython.parallel

--- per-experiment compute times typically ranging from CPU-ms to
CPU-hours; longer jobs can be run, but e.g. no checkpointing

--- rough max of 10^6 experiments per "bank" (that are to be
dispatched/cached/analyzed together); naive serial iteration for dispatch
and analysis should be feasible

--- a common target for tricky but crucial features (e.g. naive linear
interpolation to predict runtimes for parameter settings that have not been
tried; otherwise people assume many falsehoods)

In the Venture setting, authoring testcases and debugging both require
computational science workflows.

Vkm is happy to have a meeting about it.

Goals:
- (visual) debugging
- good-enough draft figures
- facilitate publication-quality figures

API to the experiment:
- Experiment is a function 
- Runner might want to know which parameters are integral, which are
  numeric, and which are general (opaque) objects (e.g., symbolic run
  modes)
- There should be an interface for controlling the initial PRNG state,
  for reproducibility
- Runner might want to know whether the function uses parallelism
  internally, or whether to parallelize around it
- Things to measure: runtime, maybe memory usage, the value returned
  (maybe offer the function a callback to which it can emit all sorts
  of different stuff?)

Features:
- Status reports every 300ms
*** Project: Get cloud infrastructure (brainstormed user stories Jan 2016)
From the eng meeting: Re: Amazon: It may actually be useful to be able
to spin up probcomp-sized Amazon machines, e.g. for optics with grant
agencies.
- also, e.g., if I want to overlay multiple 60-core runs, burstable
  Amazon machines will help.
- n.b.: Amazon has nothing quite as big as probcomp.  The biggest
  relevant things they have are a bit under $3 per hour.

User stories for burstable cloud compute

1. Batch job.  I want to run a long non-interactive job (on one
   machine) in the cloud.
   - Issue command; indicate API key, machine config, command
   - Machine starts
   - Working directory gets copied over
   - Machine runs job
   - When done, updated working directory gets copied back
     - Ideally with a transcript of the tty, and any other logs
   - Issue: Would be nice if I didn't have to keep my controlling
     process running; that is, could reattach to a running job and
     pick up the results even after the job finished and the machine
     turned off
     - This requires an always-on master for storing the results
   - Issue: Would be nice to be able to cancel a job in progress

2. Server.  I want to run a server in the cloud and interact with it,
   e.g. by browser.
   - Could be just like a batch job that opens ports and doesn't end
     by itself.
   - If it crashes, I would presumably want access to the server's
     logs, even if they weren't all in the tty.
   - Need to either control or be told the IP address or DNS name, and
     port number, where the server can be found
   - If I want to run multiple servers on the same machine, want to
     see all their ports
   - Security: presumably should default to the server being
     contactable only by me, with explicit other settings for world.
     How can that even be done?
   - Would be nice if I could fetch (or auto-sync?) the server's
     working directory on demand

3. Is there a tty interactive use case?  Can we punt that to "ssh into
   it yourself"?
*** Project: Complete the GP integration (notes as of late 2015)
Code that appears in the GP paper:
- In-text snippets on pages 4,5,6
- Figure 1, pg 7: "gpmem tutorial"
- Figure 2, pg 9: "regression with..." is neal-example*.vnts
  - "map" is also called "gradient_ascent"
- Listing on pg 12, structure-learning*.vnts
- Listing on pg 21, bayesian-optimzation.vnts

GP-related code locations:
- backend/lite/function.py
  - Definition of VentureFunction and apply_function.
  - Cute: VentureFunction objects are callable in Python
- backend/lite/gp.py
  - Vlad's original GP implementation proper.  No covariance
    functions, no derivaties.
- demos/jsripl/gp_server.py
  - Some covariance functions for Vlad's GP implementation, in
    maker-as-VentureFunction style.
  - A GP curve fitting web demo (that would be nice to keep).
- examples/tutorial-2015/regress_mem.py
  - Anthony's implementation of a general generalizing memoizer, in
    "construct my own" style.
- [deleted] examples/tutorial-2015/gpmem.py
  - A GP-specific generalizing memoizer
- examples/tutorial-2015/gpexample_plugin.py
  - Some covariance functions for Vlad's GP implementation,
    in maker-as-deterministic-sp style.
  - The backing for the GP segment of the tutorial, which should be
    maintained (but can be edited).
- test/conformance/sps/test_gp.py:
  - Tests of Vlad's GP implementation
  - Including some covariance functions in maker-as-deterministic-sp
    style.
- matrixGP repo:
  - gp.py Another GP implementation
    - claimed to be faster
    - Has code for derivatives
  - gpmem.py
    - Copy of examples/tutorial-2015/gpmem.py except for a maker-maker
      hack working around a bug in the other one
  - diff_function.py defines VentureFunctionDiff, a variant of
    VentureFunction that (presumably) participates in AD.
  - covariance.py defines a bunch of covariance functions.
  - neal_example_plugin.py, neal_example_plugin_seed.py,
    structure_learning_plugin.py, BayesOpt_plugin.py install covariance
    functions in maker-as-VentureFunction style.
  - tests/asymptotic.py and test/test_matrix_gp.py install covariance
    functions in maker-as-VentureFunction style and presumably have
    tests
  - As does the QualityMeasure directory

Bugs related to the GP project:
- #212 (do pass)
x #131 (do notation)
- [Optional, sort of] #80  (top level directives)
x #213 (model prelude)
+ #214 Alternative: can implement ref and deref as (uninteresting) macros.
+ #215 eq("foo", quote(foo)) crashes b/c VentureString is not in the type list
  - Therefore, it probably is not in the list of candidate AnyType
    values, either
  - Would need to update that list in Puma, as well.
+ #216 Should make sure that strings work as scope and block ids, (also in
  Puma)
- #217 Make there be one standard satisfactory gp implementation
  - Did we ever get good results for gradients through GPs, or is
    there reason to believe that's broken?
  - There are two versions of gp.py: Vlad's and Ben's+Ulli's.
    - Vlad's is tested in test/conformance/sps/test_gp.py
    - Ben's+Ulli's is tested in matrixGP/test/test_matrix_gp.py (?)
    - Ulli says that his version is faster and more numerically stable
    - There is also stuff involving gradients
  - Ulli says "Be aware that both structure learning and the Neal
    example won't work with the GP that is currently built in due to
    numerical problems producing inf values for logdensity.  This
    happens sometimes but not every time for Neal, almost always for
    structure learning."
- #218 Make there be one standard satisfactory library of covariance
  function makers.  See:
  - demos/jsripl/gp_server.py
  - matrixGP/covariance.py and plugins
  - Making a bunch of them be built-in constants is asking for
    namespace trouble (or long names).
- #143 Get rid of VentureFunction
- #219 Make there be one standard satisfactory general generalizing memoizer
  - There are three versions of gpmem.py: Ben's as it was in the
    tutorial, Ben's+Ulli's in matrixGP, and now Anthony's rewrite in
    terms of a general regressing memoizer.
    - The former two are the same except for a maker-maker hack, which
      Anthony's thing should now obviate (and deletion of a large
      number of comments).
    - Notably, they use the two gp.py impementations interchangeably.
  - Why do they have AAALKernels?
  - Add documentation to regress_mem to point out the hackery, and the
    possible "first package" bug
  - We want the version of regress_mem that accepts the built
    regressor rather than its constructor.
- #219 Define gpmem in terms of it (ideally returning a list of refs, but
  can do without)
- #220 Add destructuring assume (ideally destructuring a list of refs, but
  can do without)
- #221 [Option] Add iteration syntax
- #222 [Maybe a problem?] Add syntactic sugar for observe as an inference
  action to VS
  - This makes observing in a loop and observing with computed values
    work elegantly
- #223 Define the f-ing gr_ascent alias for map

Plan:
+ Add refs to Lite as macros, confirm that they work and constrain.
+ [ticket] migrate Lite refs to the model prelude once the latter is
  resuscitated.
+ [Maybe ticket] Port records to Puma and to the Lite-Puma interop, to
  add refs (and, for kicks, inference actions) to Puma
- For back-porting matrixGP/gp.py
  - Port matrixGP/gp.py back to lite/gp.py, make sure tests pass
  - Add tests of the VentureFunctionDiff capabilities
  - Back-port VentureFunctionDiff-based covariance functions
- For VentureFunction:
  - Get rid of the outer wrapper VentureFunctions
  - Add the abstraction : SP -> Python function
    - Needs to work on SPRefs, to look up the SP object
    - [Option] Make it also accept VentureFunction objects
    - Maybe with a fast path for tagged deterministic SPs
  - Teach lite/gp.py to use this abstraction
  - Port the covariance functions in the test suite and gp_server.py
    to use deterministic foreign SPs
  - Add an assert that checks that VentureFunction does not appear
  - Notify Anthony, in case it affects how he thinks about the
    compiler
  - Path 1:
    - Make sure there are unit tests of derivatives through VentureFunctionDiff
    - Make sure the abstraction does derivatives
    - Teach matrixGP/gp.py to use the abstraction
    - Migrate the covariance functions in matrixGP/covariance.py to be
      deterministic SPs
    - Add an assert that VentureFunctionDiff doesn't exist
    - Flush VentureFunctionDiff and VentureFunction

Longer-term problem:
- Ulli had to rewrite his programs in Python in order to conduct
  experiments to his taste.
  - Instrumentation of the programs
  - Compatible formats for comparison across different models and
    different inference strategies.

----------------------------------------------------------------------


axch

[My view of the paper's status]:

Everything is actually in good shape.  The outstanding work is:

- Edits vkm wanted

- Seeing whether we can simplify the inference program for structure
  learning to not restrict attention to currently present kernels

- Adding unit testing so that matrixGP can be refactored

- Adding unit testing so that the handling of derivatives can be
  ported to Venture and/or refactored, and checking whether there may
  still be bugs lurking in it.

Ulli

[yes]

axch

In that case, my prioritization for your effort would be: (1) kick off
the convergence measurement job for the simpler inference program; (2)
make the edits for vkm; (3) add the derivatives tests; (4) add general
unit testing.  I assume this was what you had in mind too, but I just
want to make sure I am sending the right signal about my needs.

Ulli

I was just typing exactly the same :D

axch

Excellent.  Now the converse exercise.  Here are things I could do:

- Improve Venture so that the code written in the paper gets closer to
  running directly

- Simplify and make more sane the GP and gpmem infrastructure (this is
  actually not quite the same)

- Try to write a nonparametric version of structure learning (generate
  unbounded numbers of primitive kernels, etc)

- Try to write a version of structure learning that doesn't need a
  back-door hack to extract the learned structure

- Try to see whether proposing parameters to unused kernels takes too
  much compute, and whether I can make it better.

Which of these things would you rather I spend my (and Taylor's) effort on?

Ulli

I think the non-parametric version of the structure learning.

as this is the only thing that I think a critical reader would pick up
directly. Especially one who read the AutoStat paper but didn't look
at their code :D

what would have been your preference?

axch

Well, I have context and plans for the two back-office activities,
namely my first two bullets above.

That was what I psyched for, though I should moderate my enthusiasm by
considering what is actually useful.

Ulli

No, go for it. The non-parametric version can wait. And I have to
think about whether it's actually useful. Above was a mere
intuition. I can also see that your and VKM's preferences seem to
align (IMHO).

axch

Have you tried a non-parametric version before, actually?  Something
you said above made me think you might have.

Ulli

not really. Thought about it a bit
*** Project: VenStan (notes as of Jan 1, 2016)
One way to push Stan: Search the mailing list for people complaining
about discrete variables
- This is relevant to Feras's thesis and he is available during break

- vkm suggestion: "Do Kepler".  Possible interpretation: model
  orbits in Stan with discrete stuff for orbit classes.

tibbetts [transcribing vkm]

1 Use Stan to do a version of Kepler that can invert more
  accurately/cheaply than importance sampling for unlikely conditions.

2 Use VentureScript to make a mixture of Kepler with an outlier model,
  to handle e.g. data entry errors.

3 With Feras, enable MML to have VentureScript expose the mixture
  indicator as an extra, unmodeled, computationally-derived column, so
  we can query on 'find the datapoints that we decided were not
  kepler'

4 Have a demo that replaces our "compare Kepler and crosscat" slide,
  to go through a 5 slide sequence that tells this whole story

An hour before 4 would be ideal, says Tibbetts

My letter on the subject, 12/22/15
The conceptualization, as I completed it, is to start with a dataset
of Earth satellites' apogees, perigees, and orbital periods, with
missing entries, and do some subset of the following tasks with it:

- Learn the variability in satellite orbits, e.g. as parameters in a
  Gaussian (mixture?) model;

- Cluster the satellites by their orbital characteristics, and perhaps
  see whether the orbit classification given in our Satellites data set
  corresponds to groupings that appear in the data;

- Predict values for missing entries;

- Detect unusual satellites, either just as a predictive probability
  threshold or with an explicit outlier model (which would keep the
  outliers from affecting the results for the other questions).

My reactions to this activity are:

- Stan is likely to do demonstrably better than likelihood weighting;

- If Stan is integrated with BayesDB, the results can be fed back and
  that promises to be useful;

- I don't see what VentureScript has to add that won't look like
  making a bear dance.

In other words, someone who has BayesDB may want to add Stan because
of this example, and someone who has Stan may want to add BayesDB
because of this example, but I don't see a compelling case for someone
who has Stan wanting to add VentureScript because of this example.

Why do I say that?  Because the discrete variables that present
themselves in this model are viewed in the Stan culture (as I
currently percieve it) as being easy and even worthwhile to integrate
out.  The Stan programming manual has a chapter on how to do
clustering models in Stan, and integrating out the "Am I an outlier?"
indicator variables is also easy (even if we are trying to infer the
outlier rate). [*] Note: Stan is perfectly happy to generate discrete
random variates conditioned on parameters fit with its HMC sampler,
so nothing prevents the model with those variables integrated out from
sampling values for them to feed to a downstream consumer.

Now, we may decide that we want to bring a dancing bear to the PI
meeting.  We may also talk around the fact that Stan has in-Stan ways
to address this particular example by saying "well, we get worse
results, but this generalizes better to more complex models".  We may
also decide that starting to play in this direction in
Stan+VentureScript pre-commits to a flexible-enough substrate that we
could relatively naturally discover a variation or extension that is
actually not implementable in Stan.  But right now I think further
investigation of problems to tackle would be a better use of effort
than jumping straight into this one (unless the twain are usefully
parallelized; battle testing Venstan will flush out problems
regardless of the task, and actually, having a pure-Stan baseline to
compare to will help debugging).

[*] Disclaimer: I have not read that chapter very carefully.  It is
possible that there may be a relatively reachable regime even here
where sampling the discrete variable does better than summing it out
in an ideal uniform implementation; but I don't see sampling in
Stan+VentureScript, with all its overhead, beating integrating out and
staying in pure Stan on a state space as small as the number of orbit
classes.
***** Letter from Vikash, 1/1/16
We have a few goals:

1. Illustrate that Venture is a polyglot platform using a non-trivial
   example

2. Demonstrate that it can be useful (either conceptually or
   practically) to address limitations of MML by overriding it (for a
   subset of variables) using Stan

3. Demonstrate that it can be useful (either conceptually or
   practically) to address limitations of less expressive languages
   (like Stan) by embedding them in VentureScript

This example will serve as a capstone example in Feras' MML paper (as
well as a useful illustration in a paper on Venture, focused on SPs).

Here is an ideal demo:

1. Implement Kepler 4 ways:

    - by hand, with importance sampling (which is what I believe we currently have)
    - in VentureScript, with importance sampling
    - in VentureScript, with HMC
    - in VenStan

   Show that these yield different time/accuracy profiles for
   conditional prediction, and compare to each other as well as the
   "default metamodel" baseline. Best results will come from finding
   an unlikely setting of the "outputs" of Kepler and conditioning on
   those; this should favor VenStan the most.

2. Implement a VentureScript wrapper for VenStan Kepler that posits a
   per-datapoint mixture of VenStan Kepler and a DP mixture of
   Gaussians. VentureScript is both handling the outlier selection
   variables and is learning the "outlier" model.

   Just show that this runs and is plausibly working, perhaps by
   histogramming the data and comparing to the inferred DPM.

3. (Important for MML paper, but not for demo): Develop an MML
   extension where tagged latent variables from VentureScript (here,
   the outlier variables) are reflected back into the GPM, so we can
   do queries to select the subset of satellites that were assigned to
   be non-Kepler, and learn about those.

If this is infeasible given time remaining and other obligations
(e.g. prep for the class) --- esp. if Feras is bottlenecked --- then
we can talk about how to best simplify this.

Vikash
*** Project: Resurrect and polish Venture-on-top SLAM
- synthetic data makes debugging easier; e.g. can make a test data set
  that has interesting dynamics in just a few frames
- suggested concrete goal: get to a test problem I can plot usefully
  with a few frames; debugging tactic: replace any offending piece of
  code with "something happens here"

- Want to test the old code to make sure I didn't break it?
- I doubt I will be able to reproduce Baxter's complicated heuristic
  for suspending inference on some obstacles.  Do we need it?
- Still need to put in the vehicle motion model

- Should check whether there is a mem key skew problem with the
  prediction of the obstacles
- Mild stupidity: As long as the variance parameters of the random
  walk are constant, it doesn't need an initialization step
- Empirically, per-row speed of SLAM seems to be dominated by
  the number of detections that occurred in that row.
- Venture still eats tremendous amounts of memory, presumably because
  of the execution history of the inference program.
  - Edit: Not any more, now that inference is untraced

Hm.  There are actually two heuristics in this program: the choice of
which obstacles to freeze, and the choice of when to devote
inferential effort to mapping vs localization.
*** Project: Flush directive-syntax map in SIVM
Possible strategy: Refactor evaluation structure (in Lite) to
carry its own origin information.  Path:
- Define an algebraic data type of abstract syntax trees
- Change appropriate elements of venture.value.dicts to emit that
  instead of the old stack dict format
- Change venture.lite.exp to expect and manipulate those
- See what breaks, and edit all broken places to use the above two
  modules
  - Can add a translation layer to venture.puma.trace to avoid messing
    with C++
  - The macro system will need to be adjusted
  - Probably all sorts of other problems will surface
*** Conceptual Bug: Non-independent principal nodes
Consider the situation of a block proposal in which the principal
nodes are not conditionally independent.  Does the prior still cancel
out of the acceptance ratio, like the system treats it?
*** Conceptual Bug: Principal node in the brush [or does brush just take it out of the principal node set?]
What if you have a proposal where a principal node
is also in the brush (because its existence is conditional on some
other principal node)?
- What does regen/detach do with this?
- What should regen/detach do with this?
- What about restore?
- This could affect gradients/hmc
- This could affect global log likelihood reporting
*** Conceptual Bug: What are the constrainability rules?
- I remember the current system's rules for what operator SPs and
  what operator-changing proposals are permitted in observations,
  and how to react to violations of such rules, as being arbitrary
  and inconsistent.
- In the code, this manifests in various corner cases of constrain
  (and unconstrain).
- Perhaps the Indian GPA issue and our choice of how to answer it
  may help clarify the confusion here.
*** Activity: Reference manual improvement
- Is it possible to set up a tracked, indexed system for displaying
  which statements in the documentation are checked how, and the
  results of those processes? (This includes decomposition of
  high-level statements into lower-level statements, like "the
  VentureScript syntax is equiexpressive and equiconvenient with the
  parentheses").
  - Extracting tested invariants from the property suite:
    - Could give every property, say, a description
    - Could instrument a run to compute a table matching SPs to properties
      about them that were tested
    - Storing the result: pass, fail, skip
    - Issue: some of the skips may be stochastic, in which case it would
      be interesting to track the rate of their incidence across runs
- Can we make the reference documentation of conceptually additive
  parts of the system be actually additive?  Preferably with
  cross-checks on how thoroughly tested those items and any claims
  about them are?  Additive referencable symbols include:
  + Lite SPs
  - Puma SPs
  + Inference SPs
  - Inference SPs that work in Puma (how can I autodetect this?)
  + Modeling macros
  x Hard-coded modeling special forms: quote, application, variable lookup, literals
  + Inference macros
  x Hard-coded inference special form: loop
  - The list of "reserved words" induced on the modeling language by inference macros
  - Functions defined in the prelude
  + Functions defined in the inference prelude
    + Maybe separate it into its own file, so I don't have to diff engine.py
  x Non-function objects defined in the initial environment (true,
    false, default, one, all, scope keywords)
  + Built-in call-backs in plugins.py
- Other additive things include:
  - Directives / Ripl instructions (include json syntax thereof)
    - Currently embedded as an explicit list in the parser, and as
      funny methods of various ripls, sivms, and engines.
  - The public Python API (for library use):
    - shortcuts functions
    - Ripl methods
    - classes returned by them (e.g., SpecPlot)
  - The Python API for extensions
    - How to define plugins
    - How to define SPs and callbacks
    - How to interact with Venture data
  - Console commands
  + Console command line options
    - [Optional] Admit more elaborate documentation than just terse help messages?
  - Data types? (and various representations thereof, notably json)
- Write actual documentation for all Venture elements (right now, +
  means "every such element has some doc, but it may be stale; except
  ones I am explicitly embarassed by, which may be omitted")
  + SPs (Lite dominates)
  + Non-macro inference SPs
    - Except "load_plugin", which is not tested
  + Modeling special forms
  + Inference macros
  - Functions defined in the prelude
  + Functions defined in the inference prelude (incl: pass)
  + Non-function objects defined in the initial environment (true, false, default, one, all, scope keywords)
  + Built-in call-backs in plugins.py
- Things that are not referencable symbols but ought to be documented regardless:
  + Directives / Ripl instructions
    - Except "load", which appears to be broken?
  - The public Python API (for library use):
    - shortcuts functions
    - Ripl methods
    - classes returned by them (e.g., SpecPlot, Infer(!))
  - Console commands
  + Console command line options
    - [Optional] Write more elaborate documentation than just terse help messages?
  - Data types? (and various representations thereof)
- Add cross-references among all the program elements.
- Spell "quasiquote" and "unquote" sensibly in the documentation, but
  do not lose the example use case (or the fact that quasiquote works
  in model expressions too).
- Should really nail the words and story for tagging (scope_include) by the release
  - Related idea from Will Cushing: Maybe make a default tagging
    scheme based on existing variable names and procedure arguments
- vkm likes doctests: "I like the idea of a registry of content bits
  that is programmatically assembled, so that someone who writes an SP
  can locate an intro use case 'in the comments', and the code +
  results appear 'in the docs'"
- vkm suggests that error messages and profiling can be kept more alive
  by being made into "example-documentation-generating self-test cases"
- Emit, in the documentation of each SP, a table
  describing its usability in each position (principal
  node, internal node, border node) for each inference method
  (resimulation_mh, gibbs, slice, hmc, rejection) in each backend
- Also notes about which inference methods are available
  in which backend
- Could potentially set Jenkins up to push an updated edge reference manual
  to the web on every successful smoke build.  This needs:
  - A CSAIL machine account for Jenkins
  - Adequate storage for credentials thereto
  - A Jenkins job that actually does it
  - A note in the Jenkins setup tool about how to set that up again if
    we lose the Jenkins config
- Could split built-in stuff into modules (with an "import" command),
  and reorganize the reference manual by module.
  - This makes room for modules of different degrees of "stability",
    like sticking Wadden's permutation hack into a module.
  - Also, the vector nonsense has a chance to develop if it lives in a
    module of its own.
  - There's a choice of what selections from what modules the prelude
    re-exports.

Notes for the future of the reference manual:
+ Actually publish the reference manual, so people can read it
- Automatically update the version number that the built documentation sees
- Make cross-references in the documentation work
  - What is the Sphinx-ism for that?
  - Are there any namespacing issues?
- Can I get back the symbols +, -, <=, etc, in the generated manual?
  - Probably the easiest thing is to just flush the operator renaming thing
- Would be nice to autodetect and add to the documentation which SPs
  have the metadata necessary to participate in which transition operators
  (subtle, because it depends on whether they are principal, crg, or absorbing)
- Would be nice to autodetect and add to the documentation which SPs
  are ok to observe (subtle!)
- Would be nice to migrate existing comment documentation to places
  where the reference manual can refer to it:
  - How to use SubsampledMH
  - (needs to be written) How to write dynamic programs with enumerative_diversify
- A nice exercise would be to extract the subsampled MH stuff into a
  module (so it can have its own darned reference manual, and not
  confuse the bejesus out of normal users)
- Perhaps I could move the macroexpansion target SPs into a module
  too, so they do not clutter the main presentation.
*** Activity: Thoughts on release polish circa late March 2015
The activity of release polishing is to look at the system from the
outside in and fix what's broken.
- Does the reference manual explain all the features of the system?
  - Including the programmatic API?
  - Including how to make SPs of all the various breeds?
- Does the reference manual document all the cross-feature
  interactions?  For example:
  - Which SPs are available in what backends (with the same behavior?)
  - Which transition operators are available in what backends?
  - Which SPs are usable in which positions of scaffolds for what
    transition operators?
  - Which SPs in what circumstances will impede serialization or
    deserialization (and where is serialization implicitly used)?
  - Which Puma operations will barf on what kinds of Python SPs when?
- Are the error messages that occur when one hits some corner case
  clear and helpful (see list of cross-feature interactions above)?
  - Do we want to compose a glossary of errors?
- Do the tutorials/demos advertise all the features we want to expose?
- Are all the examples up to date, and is it clear what they are
  exemplifying?
  - Should I exclude examples/notebooks from the release, since they
    are stale and hard to keep fresh?
- Is it clear which subsystem to use when and for what?
- Are the installation instructions simple and reliable?
+ Flush the old c++ backend
+ Update most of the license headers (also copyright years, perhaps)
- Finish updating the license headers, if desired.

More detailed punch list:
- SIVM: names of venturescript and metaprob, in source; names of puma
  and lite, in source. just top-level README (and perhaps also
  backends README and console README)
- IPPE: minimal doc (just pointers to "They exist", with a .vnt
  example) and discoverability for plotf, callbacks, ... -- plus
  console docs
- "Online Tutorial": script for new demos (vkm will sketch text and
  make exercises for Suresh)
- Model Library: SPs so far, plus various .vnt files, with a read?;
  weed examples so that they run, with a readme, and rename Model
  Library
+ There is an issue with deprecations: do I rush to do them for the
  minirelease, or do I accept doing them immediately afterward
  (generating spurious non-compatibility).  Cases in point:
  + scope_include remaining an alias for tag
  + scope_exclude
  + loop taking a syntactic list rather than a single action
- Rerelease
  - Maybe bump the version number, if enough changed; incl. in the reference manual
  - Maybe rebuild the reference manual, if changed
  - Maybe reupload the reference manual, if changed
  - Rebuild the tarball (maybe recheck exclusions)
  - Reupload the tarball
  - Rebuild the docker container
  - Reupload the docker container
  - Make a new section on the front page, if version number changed
  - Update the sha1 sums (index, container instructions)
  - Rebuild and reupload the front page
  - tag the release as release-foo

Content to polish:
- Read the manual and make sure things are interlinked properly by
  choosing between single backticks and double backticks, and adding
  roles where needed.
- Sphinx warning nitpicks:
  - There is actually a name clash between inference repeat, which
    repeats an action, and model repeat, which fills an array.
  - There is a "name clash" between inference print and model print.
  - The four modules are not listed in any toctree

Other stuff we thought circa Jan 2015 that would be nice to have for release v0.3.1:
- Mini tutorial on inference programming (IPython? impose on vkm?)
  - "Like the Classic Bayes article, but executable"
- Would be nice to have a website where one can see some Venture
  programs (maybe statically generated)
- Ideally include the profiler in the release
*** Activity: More testing (e.g., log density agreement; see Asana)
- Look for known (unremembered?) small bugs and confirm intended
  invariants.
- No doubt we have various asymptotic performance losses now.

Mechanical test idea:
- check that permuting order of incorporation does not actually affect
  the answer
- check that unincorporating (even out of order) actually produces the
  same result as not having incorporated in the first place
*** Activity: Cross-port model/inference SPs
- Port Puma-only model SPs to Lite
- Port Lite-only model SPs to Puma for efficiency
- Remove discrepancies evident in exclusion lists in test_properties
- The inference SP part is mostly about porting Lite things to Puma
  - Do we want to shrink the trace interface so that all the inference
    methods are written just once in Python?  Will crossing the C-Python
    boundary at every regen kill performance?
  - Can we take an intermediate position and run Python inference SPs
    just for the things that have not been ported to Puma?
    - Will this just work if I make the obvious plumbing?
*** Activity: Fill in the Python SPs in Puma interface (caveat: it is not thread-safe)
*** Activity: Fill in conjugate and collapsed models
*** Activity: Tutorial content polish (notes from the prep push, summer 2015)
Features that would make the teaching material nicer:
- Do we have quote, quasiquote, unquote shortcuts in venturescript?
- The do pass bug
  - Option: Adjust do to delay the expression or statement as well,
    and bind and bind_ to return an action that forces the thunk
    - Con: Bare bind will be annoying to use
      - Option: Define alternative bind and bind_ for the benefit of do
  - Option: Document the phenomenon and tell users to work around it
    if it's a problem
    - e.g. by do pass
  - Option: Eta-expand do with a worker-wrapper pattern, and tell the
    wrapper to add a pass at the beginning
    - Con: hand-macroexpanding a do into a let something do will be
      wrong because it will move the pass
  - Let case: mapM (do (something <- (normal 0 1)) (return something)) and 
    check distinct
- do-let
- fork and patch ggplot
- Provide a way to make labeled instructions from the console, better
  than [infer (observe ...)]
- Fix or flush the darned log score computation in collect
- Would be nice to be able to plot the raw data set with plotf
- [Optional] Add interaction logging (console? file execution? ripl API?)
  to local file, which we will be able to read afterward

Optional features that would make the teaching material nicer:
- Analytical expectations overlays for plotf: Is this a Gaussian with
  a particular mean and variance?  Is this a Gaussian at all?  What
  evidence do you have for or against?
  - Possible interface: an SP that accepts the dataset, does whatever
    it wants, and returns a nullary SP purporting to be the data
    generator.
  - This also gets into q-q plot territory
- Could also make do with empirical expectations overlays, where the
  only thing requested from the context is samples
- Faceting in plotf (might be easy: ggplot allegedly has facet_wrap
  and facet_grid)
- Fix the visuals of histogram with color-by in plotf
? Emacs mode for venturescript with syntax highlighting
- A combinator for making visualization plugins that calls your
  procedure with every row of a data set and gensymmed file names, and
  then animates or layers on disk.
- Issue: infer posterior returns VentureNil, which renders as [] on
  the console, and is confusing.
  - Option: Make it return the number of trials, like I wanted
    - Then will need to explain that
    - Theoretically will need to return either the total or the list
      of results from the loop in trace.py
  - Option: Find some way to suppress the nil in the console (which of
    course is only really valid if the return type is void, namely
    that it never returns a non-empty list; which there is no obvious
    way to detect)
- Move the standard columns in Datasets to the end so the data comes first
- Idea: Teach plotf to have the number of bins in a histogram be the
  square root of the number of data points (with a minimum of 20 or
  30).
- Issue: "Key Error: 'default'" is not helpful when doing inference
  with no model
  - Option: Detect it and emit a more helpful error message
  - Option: Allow it to proceed and do nothing
- Idea: violin plots of chain position vs iteration count would really
  show how the state distribution evolves, without getting tangled up
  with the history of each chain.

iVenture may well be the right thing for transcripts.
- I personally don't like it because the editing interface is not
  Emacs

More features for the transcript mechanism:
- Select between interactive and batch operation in the source
  transcript.  Differences:
  - return values are printed interactively but not in batch mode
  - batch mode presumably starts a new ripl, rather than continuing
    the existing session
  - the console intercepts list_directives and turns it into
    ripl.print_directives
- Option: Detect whether the plotf substitution probably broke
  - If plotf is abstracted
  - If plotf is called in a loop
  - If plotf is called with multiple plot specs
- Alternative: Redefine plotf in the environment as a thing that calls
  back to the transcript runner to request and update the current
  figure count.
- Style the output reasonably
- Distinguish Venture commands from output, while preserving
  pastability
- Would be really sweet to get the right syntax highlighting in the
  output!
- Can I convert looped graphics rendering into a prebuilt video?
  How would I hack that?

Later:
- Issue: Grem thinks explicit models are clearer.  He may now be right.
  - Idea: make assume model_name.x (normal 0 1) essentially the same as
    (in_model model_name (assume x (normal 0 1))); that way, can
    conveniently live with all-explicit model namespaces.
- Add begin as a sequencing combinator for effectful functions (which
  just strictly accepts their return values and returns the last one)?
- Idea: Convert plot specs to a structured object, since that's what I
  meant anyway (maybe not for the summer school).
- Idea: Could add vectorized dependency tracking, e.g. noticing that
  the dependence between this vector and that one is per "map"; then
  might be able to optimize changing just one element of a vector
*** Activity: More profile-driven optimization
Profiling 30 steps of mh on a 21-point linear regression example from
the Venture tutorial [+ means "fixed it"]:
+ 25% is spent in scipy evaluating logpdf of normal
Of the remainder:
+ 13% is spent generically copying Args structs in unwrap_args
Of the remainder:
+ 18% is spent making Args structs in the first place
  - Do I want to cache the Args object in applyPSP or such?
+ ~10% is in checking that every value extracted from the trace is appropriate
+ To fix up args structs
  + Make an explicit class of UnwrappingArgs for SPType to produce
    (instead of copying)
    - Testing on the lin reg test case indicated that only
      operandNodes, env, esrNodes, operandValues, and esrValues were
      actually needed
  + The requestValue, esrValues, esrNodes, and madeSPAux fields were
    only added to Args of OutputNodes
  + Change the interface of operandValues to a method, and hide the
    lookups in the body
  + Figure out what the right thing is for new_args.operandValues in
    DefaultVariationalLKernel
  + Change the interface of esrValues to a method, and hide the
    lookups in the body
  - Cache the operandValues method?
The remainder, unprofiled, takes about 4 sweeps = 120 transitions per
second.  Of that:
- ~40% is spent on rejections (does that involve computing log
  densities, even though the answers are thrown away?)
  + Could try func_mh a la Puma
    - bug: Lite particles currently do not support detach
    - didn't help anyway; our wt tree implementation is too slow
- 15% is spent in trace.pspAt
- 15% is spent in applyPSP
-  9% is spent constructing scaffolds
  - If I make them functional (by moving the mutating regen counts
    out), I could add an LRU cache (on which various interpretations
    of the scaffold could later be hung)
    - e.g., regen is actually a list of instructions like this:
      - call this method of this psp on these arguments
      - store the result in this name
      - look this value up from the trace at this node
      - store this value in the trace at this node
    - could do a version of that where each family has its own such
      instructions, possibly with conditionals (that depend on which
      control paths are active)
*** Activity: Cull Asana, fix/complete or record those bugs/projects
*** Cleanup projects that are relatively farm-out-able
Selection criterion: feature-driven, fix-driven, or refactoring-driven?

Issue #294 (Unparser for VentureScript)
Issue #56 (operator substitution)
Issue #57 (modify_expression)
Issue #58 (observe_field dict)
Issue #358 (parallel PSP tree)
Issue #359 (stack dicts to namedtuples)
Issue #362 (test counter-example minifier)
Issue #360 (pull plotf out as a (standard) plugin)
Issue #59 (flush transient trace support)

Foreign inference SPs as a registry:
- Notionally, making the inference sp dict a registry will be trivial
  when I drop support for transient inference traces
  - Could also permit transient inference traces to have the bug of
    picking up all foreign sps, independent of importing
- Once they are both registries, I can merge them into one registry
- Share register_foreign_sp across bind_foreign_sp and bind_foreign_inference_sp
- Consider whether foreign imports should be directives, and/or forgettable
- Add import_foreign for the inference trace
*** Paper: HMC in Venture (notes circa June 2014)
- There is a draft manuscript in
  venture-documents/gradient-based-inference/gradient_nips2014.tex

- There are programs that generate some figures there too:
  figures/motivation.py (squiggles); figures/logistic.py

- There is an abstract in nips-probprog-2014-abstracts/gradients; more
  math in comments in that tex file
***** Loose threads from HMC paper project as of 6/6/14
Audience assumption:
- the doubts about prob prog are about whether it can do X
- but people do not already know what prob prog is

One way to phrase the proof obligation:
- Determine the joint distribution on the variable of interest and
  all additional quantities computed by the algorithm

- Prove that the marginal of the proposed value under that equals the
  target, assuming the initial state is drawn from the target

- Argue that "the chain is irreducible and aperiodic (Tierney, 1994)"

----------------------------------------------------------------------
"A page ish" for the math

Terms:
- joint density conditioned on the torus (is proportional to the conditional posterior)
- the marginal density of "principal node" (principal variable?) under the joint (or given the joint find the marginal)
  - "target variable(s)"?
- "conditionally random variables"; "conditional randomness"
- "constrained choices" = "absorbing border"
- "foo" density: mixed over the RNG
  - "forced" density; "derandomized" density; density for (partially?) derandomized model fragment
- gradient of that
  - subtlety that gradient of log density calls gradient of simulate

- points of comparison:
  - run the whole program every time
  - dig up the reference for "monte carlo estimation of the gradient is ineffective"
  - maybe also compare empirically what happens if one doesn't do the mixing thing
    - vkm says "Any smart thing should have its geometry-dependent
      convergence properties wrecked by gradient noise"
  
- the proof may tell me that I need to regen an extra time with free randomness

vkm invented a great example:
- get a monte carlo ray tracer; target nodes are continuous scene
  coordinates; likelihood free SP is a MC ray tracer (with continuous
  pixel values and a Gauss comparison at the end)

Possible bug, at least in the presentation: scopes/blocks permit the situation where
some principal node is a child of something in the drg or the brush

Caveat: all the principal (target) variables must exist (not be in the brush)

----------------------------------------------------------------------

vkm says:

- Goal #2: Demonstrate applicability of our method to standard online
  learning problems over continuous spaces; needed to cement our "real
  application" worthiness, and to be done by Alexey in case bugs with
  SMC are flushed out

- Buys insurance if DPM of logreg doesn't give serious enough looking
  results

- Raw model:

  [ASSUME w (multivariate-normal 0 (* sig I) 10)]

  [OBSERVE (bernoulli (logit (dot w <x_0>))) True]

  ...

- Use FORGET and [INFER (map ...)] to implement a variant of
  stochastic gradient descent, where gradient steps are taken with
  respect to single data points (or small batches of data points) at a
  time, and the prior on w is broad enough to hopefully not matter

- Test on a simple synthetic classification task; Terry can turn it
  into a more real one later

- deliverable: could generate data from a log reg and measure error in
  weights; also predictive error (as a function time spent computing)
  - for batch size
  - Yutian allegedly did a digit classification task on a standard data set


- Goal #3: Illustrate "fancy" example with robustness of our AD scheme
  by applying it to a toy inverse interpretation problem

- Perhaps most optional, but would make the paper much stronger; a
  less-clear writeup with this example is likely to fly better in
  review (though not what we want for camera ready)

- Worth spending a few hours on our inverse interpretation test case
  to see if it works at all, and if yes, consider continuing

- e.g. deliverable: one fit curve, graph of predictive accuracy over held out data


- Goal #4: Work with VKM to revise the writeup
***** Email discussion late Jan 2015
vkm to axch:

1. ROUGH INTRO/EXTENDED ABSTRACT

Gradients are a key building block for algorithms that solve
high-dimensional optimization and sampling problems. In principle,
automatic differentiation can be used to generate high-quality
implementations of gradient-based inference for probabilistic
programs. Unfortunately, existing approaches cannot recover many
standard gradient-based algorithms and only apply to restricted model
classes.

This paper describes a new formulation of automatic gradient-based
inference for higher-order probabilistic programs that addresses these
limitations. It supports batch as well as stochastic formulations of
optimization and Hamiltonian Monte Carlo, and handles stochastic
energy functions via auxiliary variables. The paper illustrates its
flexibility by using it to implement a novel Hamiltonian Monte Carlo
technique for approximate Bayesian computation called ABC-HMC, as well
as gradient-based particle methods for models with unknown numbers of
objects. Empirical results are provided for parameter estimation in
state-space models, online logistic regression, and Dirichlet process
mixtures of logistic regressors, each using under 20 lines of
probabilistic code.

2. MINIMUM FEATURES AND EXPERIMENTS

- proper linguistic control of "stochastic" vs "batch" mode for
  gradients
- (copied) conceptual ABC-HMC illustration
- online (stochastic gradient) learning for logistic regression
  refreshed and run on MNIST subset (subsampled way down if needed);
  graph shows online vs batch comparison in terms of "1 online sweep"
  time
- DPM of logistic: copy from Yutian; get running with gradients on
  simple test; show that accuracy improves (at higher computational
  cost) for MNIST, vs Gibbs and SMC
- "resample-move" Bayesian parameter estimation for state-space models
  (filter on latent Gaussian state, using a rejuvenation kernel for
  the parameters): show on synthetic, vs pure SMC, and MH
- "it does not crash on seismic"
- some "real world" version of one of these (possibly try to scale up
  MNIST?)

3. HIGHER QUALITY EXPERIMENTS

- puma implementation of gradients
- show it runs on SLAM (for mapping, and possibly also for
  localization, perhaps assuming offline gradients for the sensor
  model)
- show it works well to help scale up e.g. Seismic
- "10x" larger scale for other experiments
- other statistical models in supplemental

4. REAL OBJECTIVES FOR USEFUL GRADIENTS

- auto-documented coverage with seatbelts for naive users and fuzz
  testers
- user-specified gradients
- makes ~1000d continuous models practical
- enables test of learning tree-based models via SMC/MCMC + gradients
  for the continuous parameters (for phylogenetics)
- handles "screw" cases we need for future papers: learning programs;
  BLOG-style models; ...
- "fast enough for standard ML research on standard ML problems"
- "can assess coverage and performance vs Stan, to incite someone to
  integrate Stan and expand our quality suite"

----------------------------------------------------------------------

axch to vkm:

This message is bullet-wise reactions first, and a summary at the end
(which I suppose you can read first).

MINIMUM:

What are "batch mode" and "stochastic mode" for gradients?  If this is
about playing games with incomplete scaffolds, getting it to work is
likely to be a project.  If this is a "rolling observe/forget" hack,
then it will have weird interactions with any observation function
that has latent randomness; which we might be able to punt by writing
the Venture program carefully.

I assume "conceptual ABC-HMC illustration" will be pretty easy (unless
it has to actually run?)

Is "online (stochastic gradient) learning for logistic regression" one
of the things I did for the HMC abstract?  I am confused about how
those words describe that artifact.  In particular, there is no
explicit randomized subsampling of the input going on there (though
I suppose such could be arranged externally).

Do we actually have a DPM-LR model that works?  If such a thing bit
rotted, re-debugging it could take days.

You say "show that accuracy improves (at higher computational cost)
[vs another method]".  How is it possible to show that?  Aren't all
these things time/accuracy curves that asymptote at "perfectly
accurate"?  Or do you want to talk about accuracy & cost vs sweep
count?  In any case, we can't do principled wall-clock comparisons,
because Venture's overheads are so severe that even minor inequality
in the distribution thereof across methods could easily swing any such
comparison.

Is the parameter estimation thing essentially what we had for the
abstract?  If so, getting that to work is just about "debugging a
sampler".

Seismic: Do we even have a seismic that does not crash?  My model of
it is that David was off in some weeds -- have we shipped anything
there yet?

Getting to actually real-world is likely to be impossible in two
weeks.  What model size qualifies as "real-world" with the scare
quotes?  Pushing this dimension is an open-ended time sink.

HIGHER QUALITY:

Wait, are you implying that Puma gradients may not be necessary for
all the "minimum" objectives?  I thought the essential reason we
didn't submit to NIPS was that even Terry's Puma port was too slow to
run examples with non-laughable sizes.  There is no reason to expect
performance to have improved since then (except for the
multiprocessing thing).

What's needed for SLAM?  A gradient of the laser SP?  (In addition to
unscrewing the SLAM program, which is work that has co-benefits).

REAL OBJECTIVES:

Coverage is orthogonal to all the above.  What do you mean by
seatbelts?  Some fuzz testing we already have; more either will arise
naturally or is orthogonal.

If you mean through the foreign SP interface, we already have
user-specified gradients.  If you don't mean that, it's orthogonal.

Screw cases are orthogonal, and serious.

The rest of the list reads like it's about performance.  I expect the
first 10x to 100x of performance improvement to have nothing to do
with gradients per se (except to port gradients to the performant
backend), and to take a week to a year.

OVERALL:

Reading the above, I do not feel good about aiming this for Feb 6.
Unless the minimum is easier and more acceptable than I think,
probability of success strikes me as low.  I am also on the
fence about path:

- Option a: Start doing the "minimum" things in Lite, and hope that it
  proves fast enough to have a reasonable debugging cycle for them.

- Option b: Start by getting and debugging faster gradients, and hope
  that doing so leaves enough time for experiments and runs.

Starting (a) and switching to (b) will be very disappointing, because
doing (b) first would have made (a) more pleasant (perhaps
substantially).  Option (b) also seems like it will have better
cobenefits.  However, (a) has a chance of getting a paper along the
minimal lines above out by Feb 6, whereas I think (b) does not.

My pessimism may be alleviated by a positive surprise from
parallelizability of tasks across team members, but I am skeptical.
Taylor is presumably quite valuable shipping BayesLite, and in any
case will need time to spin up on AD and on Puma if he is to do that.
Baxter could be of help debugging the samplers, but unless he and I
are pairing it, his inexperience with Venture is likely to be a major
hindrance.

Actually, the most fun way to do this project would be for me to pair
with Baxter on the debugging aspects -- it would teach me about
debugging stochastic nonsense, and it would teach Baxter about
Venture.  I worry, however, that doing it in Lite will be an exercise
in small talk while the examples are running, and doing it in Puma
will fail because Puma gradients will not be ready fast enough to
submit on Feb 6.

I think a synchronous conversation would be helpful.  Skype?  Phone?
I could theoretically come in to the office, but Lev looks like he
might be sick again, so if it's avoidable I'd rather not.

----------------------------------------------------------------------

vkm to axch:

Hmm. OK. Very useful. Based on this I'm leaning towards delaying, in
favor of quality rather than quantity of submitted papers --- though
only in light of a plan that will let us ship it this spring.

Next iteration: let's posit the opposite kind of plan: work on what we
Really Want, and when it's far enough, pause to put something on the
arXiv, and hopefully also submit to NIPS. The right answer may be
somewhere in between.

- Even if our gradients are too slow for ML apps, we would greatly
  benefit from having a version that is usable for "screw cases" where
  the overheads are mostly unvoidable already, e.g.:

  - inverse compilation
  - BLOG-type models (which Wadden got running, actually :) )

We would also want to verify that the asymptotic scaling is correct,
and add support for user overrides of gradients.

If we have these, usably, then gradients would feel far more useful to
me from the overall project standpoint. What's involved here?

- What is the best way to get Puma gradients implemented? Could Baxter
  do it as a learning project, given enough time, and a skeleton
  written by you?

- Would gradients be another good test-case/warm-up for a compiler? If
  so, this might be sufficient reason to delay until NIPS.

***** Reading notes on references
Girolami and Calderhead, Riemann manifold Langevin and Hamiltonian
Monte Carlo methods.
- This deals with the mass matrix of HMC.
- Every step seems to cost O(D^3), because the proposed symplectic
  integrator is implicit.
- How, actually, does one choose and compute the metric tensor?
  - p. 7 "It should be stressed that the MCMC methods which follow in
    this paper exploit the Riemann geometry that is induced by the
    metric defined by any arbitrary positive definite matrix G(theta)
    and the practitioner is completely free in this choice."
  - Example options include
    - the expected Fisher information matrix (matrix on theta)
    - the observed Fisher information matrix (which is said to be the
      negative Hessian of the log-probability at a particular point
      (of theta))
    - the empirical Fisher information matrix
    - "The examples that are reported employ the joint probability of
      data and parameters when defining the metric tensor"

Hoffman and Gelman, The No-U-Turn Sampler: Adaptively Setting Path
Lengths in Hamiltonian Monte Carlo
- This deals with the trajectory length, and explains primal-dual
  averaging to set the step size.
- The "convex optimiation problem" in this case is to minimize the
  difference between the observed acceptance probability and a target
  (typically 0.65 or 0.6), as a function of the selected step size.
- The optimization strategy is alleged to be an instance of Yurii
  Nesterov, Primal-dual subgradient methods for convex problems,
  presumably on a 1-dimensional function.  Did not actually read that
  paper.
- The experimental section gives explicit numbers indicating the
  computational load of producing their figures.  A possible
  "benchmark": How much CPU time would we need to replicate their
  test?
- Issue: the adaptation of the step size proceeds over many
  trajectories, making it (somewhat) more suitable for an actual MCMC
  scheme with long chains than for disconnected steps whose purpose is
  to define a distribution.  But perhaps the interface could be
  adapted to allow reuse of past adaptation.
- Confusion: They blamed the need for an acceptance step in HMC on
  energy non-conservation in the integration, whereas my reading of
  Neal seemed to indicate a different reason (does this have to do
  with treating the momenta as persistent or transient auxiliary
  variables?)
- Made some comment to the effect that combining No U-Turn with
  Riemannian may be nontrivial, because the actual "no U-turn"
  condition may be subtle in a non-Euclidean space.  I assume it's
  possible to use literally the same condition, but its heuristic
  value may be diminished.
*** Paper: Notes (late 2014) on camera-readying the probprog NIPS abstracts
"Abstracts can, when viewed from more remove, form a more complete
 picture of the overall enterprise."

Details to attend to in the texts:
- uniform conventions for unspecified syntax
  - (nesterov <scope> <block> rate=<rate> steps=<steps>) ; no transition count
  - (exact) = (rejection default all 1)
  - (optimum) = (emap default all 1)
  - default all is default scope for nesterov
  - default one is default scope for mh
  - in venturescript, cycle(<count>, k1, k2, k3)
- make decisions about unspecified syntax
  - rename uniform_continuous? c_uniform? uniform?
  - predict = inspect
  - scope_include, with-tag (the latter takes one argument and nests?)
  - selectors
  - keyword arguments
- citations?
  - grammar of graphics
  - old Venture paper
- consider author order

- Add references (esp. gradients)

- Do not lose the nice exposition of gradient math I did for the gradient abstract

- Note: It is good practice to make experimental programs fully
  reproducible by setting the seed

- Reproducibility of papers - fixed seeds (?), git submodules, git
  history coalescing
  - Check squiggles and log reg into the abstracts repo?
***** Waiting for Dan Roy to forward the reviewer comments, since 12/13/14 :work:
***** Plotf
- Vlad says: explain what resample is doing?
- Maybe show trick coin migration to posterior?
- Tejas says: Bigger x-ticks.  Is this applicable in general?
***** Gradients
- Disappointment: we don't actually have any examples with brush, do we?
- pi* can be viewed as a stochastic estimate of the true local posterior pi,
  the difference with stochastic gradient methods being smooth biased
  streamlines rather than unbiased squiggles
- can do pi*_k for more compute; math still works
? do inference on the brownian parameters (cycle nesterov?)

- motivation.py:show_samples appears to have a bitrot bug involving
  list directives.  Could there be a problem with directive numbering
  across TraceHandler stuff?

- compute budget in logreg counts the time it takes to observe the
  batch; but the signal that appears on the graph does not
******* Notes for possible further growth (from vkm Oct 17, maybe outdated Oct 22)
- Summary:


Gradients are often useful for solving high-dimensional optimization
and sampling problems. This paper describes a formulation of automatic
differentiation and gradient-based inference for higher-order
probabilistic programs that handles arbitrary stochastic
computations. This formulation overcomes key expressiveness and
scaling limitations of previous approaches. For example, it applies to
"likelihood-free" models, supports higher-order
probabilistic procedures, permits interleaving of automatically
generated and user-specified gradients, and can be used for online
inference in standard models from machine learning. This paper also
presents preliminary results from a prototype implementation using the
Venture platform.


- Figures:

  - Core math + idea:

    (a) define AD for stochastic functions: show box diagram for usual
    AD, then reify randomness inputs

    (b) give recursive rules that suffice to define AD, including
    invocations of gradient-of-simulate

    (c) give example of user-specified gradients defined in the
    language (v1?)

  - Randomness issue: use HMC for noisy-normal, overlaying plots to
    highlight global vs local tension

  - Benefits: 

    (a) probabilistic code + eqn for online logistic regression

    (b) probabilistic code + eqn for Gibbs+gradients on a DPM of
    logreg experts (cf Yutian)

    (c) time-accuracy curves vs MH, from line, for (a) and (b);
    ideally contrast with rerunning the whole program over and over,
    to drive home the inadequacy of randomdb, on both (a) and (b)


- Notes:

  - workshop should be focused on idea, doable with lite

  - AISTATS (11/1) will require PyPy / Puma for "serious" scale;
    ideally also a stan converter tested via Geweke

  - optional, but probably crucial for AISTATS: graphical model +
    prob. code for particle filter with HMC rejuvenation (stochastic
    volatility?)

  - optional: add comparison of the effect of # auxiliary variables on
    the convergence rate of MAP vs HMC

  - optional: we can show an example where this is intuitive and where
    this is subtle, e.g. gradient and implementation

  - what is the quickest path to a draft here? what should we pair
    (e.g. the math?)
***** VentureScript
- Might be able to seed both runs the same, so they get the same prior sample

- Could do mechanical transformation from program that exhibits
  concrete semantics to one that exhibits ideal semantics.  Possible goals:
  - Save ourselves time and embarrassment keeping two related versions
    of a program actually in sync
    - An external, unexplained source-to-source translation will work
      for this
  - Point out that the relationship is formal
    - An English or pseudo-code source-to-source translation will
      probably work for this
  - Claim that we offer our users access to the ideal distributions as
    a debugging aid
    - The artifact needed for this claim depends on how much
      integration we want to claim, and how much production
      finishedness
    - A ripl method or console command that implements a
      source-to-source transformation or a dynamic rebinding of the
      meaning of INFER might do
  - vkm says the purpose of a program is to make the machine-learning
    audience treat the relationship as formal (without inventing a
    bunch of mathy notation)
***** Core Venture
- fig 4 can we do side by side and contrast?  Explain that "exactly" is assessable?
- try to compare the definitions of gamma on one page, and punt the assessor and plot to the next?
- parens rather than brackets? mention "model" in fig 1?

- An exchangeable SP that's using a trace to store its state
  - Aesthetic stake in the ground
  - Somewhere, there is a semantic hook for demanding a model for all mutable state
  - Alexey replies: if you are thinking of writing a trace that swaps out
    creation of subtraces, you might as well write one that swaps out
    creation of any other mutable objects too
    - can view mutable boxes as funny kinds of traces that have very
      little extra flexibility
    - but we can also demand that such boxes come with "priors",
      especially if we are willing to supply a default that admits any
      representable object
- A stochastic search inference program, e.g.
- Use a model to help with inference (does monte carlo tree search
  look like that?)
- Write a custom MH procedure for some model without having to repeat
  code for sampling and assessing
- Example of generating data until the reasoner reaches a certain conclusion
  - The theory: there is a procedure from prior prob and list of
    flips that emits an estimate of prob of trickiness.  Can be written by:
    - simple monte carlo over inference
    - analytically
    - as an emulator via logistic regression from data generated from inference
  - Check that at least the first two agree when used as subroutines
    in the program that tries to pick how much data to have
  - The stuff is in show-off.scm
  - version two: estimate (automatically or manually) to check that
    the reasoner is convinced
    - can maintain an explicit representation of the model program
      instead of trying to incrementally grow it (can also take #
      transitions as an argument)
    - can cross-check against the true posterior
  - Scatter plot: # trials it took to be convinced vs prior prob that
    coin is tricky (10 samples from approx posterior to generate
    estimates of degree of convincedness)
  - The full posterior is probably also given by enumerating over
    is_tricky in the collapsed version of this
  - Emulate the behavior of the is_tricky inference from features prior
    prob + #trials + %heads by logistic regression, whose training data
    generate from the Bayesian model; can use the previous thing to ask
    for points that cover the space (of is_tricky)

- [duplicated vs Make a good v1] Bug with the v1 Marsaglia gamma
  program: the second rejection reruns the whole program, and does not
  reject even when the exactly fails in the first observation.

***** Stochastic Procedures [punted]
******* Notes from vkm from Oct 17 (maybe outdated by Oct 22)
- Summary:


Subroutines provide a simple, language-independent way to encapsulate
fragments of programs, thereby supporting reuse, testing, debugging,
interoperation, and optimization. Programmers frequently test
subroutines in isolation, write programs that invoke subroutines
written by others, and incrementally optimize programs by rewriting
performance-critical subroutines in lower-level languages. This
abstract describes {\em stochastic procedures}, an analogous means of
encapsulating fragments of probabilistic programs. It gives a
mathematical specification of the interface that stochastic procedures
must provide, contrasting them with the elementary and exchangeable
random procedure notions from Church. Hidden Markov models are used to
illustrate their performance engineering benefits. Finally, a testing
technique for stochastic procedures is described.


- Figures:

  - Interface: what is required for rejection, importance, MH (undo
    procs), latents, exchangeable coupling

  - Illustration of HMM "trace fragments": (a)
    compound + use (b) external collapsed (c) external w/ LSRs

  - Benefits: (a) speedups for HMMs (b) code for Geweke w/ general SPs
    (c) Geweke results, e.g. Q-Q


- Notes:

  - I have Geweke code that should be easy to port

  - vs XRPs, talk about external latent variables, and symmetry with
    compounds. Also talk about metadata as code, for declarative specs
    (densities), but also gradients,.

***** Stochastic Regeneration [punted]
- regen and detach and construct scaffold (minimal re-simulation and minimal re-execution)
  as primitives (likelihood-free/deterministic)
- v1 implementations of one or two inference stategies (m-h,
  rejection, egibbs)
- [optional] partial meanfield would be great
- [optional] some dynamic program special case
  - By splitting egibbs into multiple calls to regen (presumably only
    works with special structure)
  - Maybe on a discrete-state HMM
- Example: trick-coin, because it will show differences between the methods
  - [maybe] could write it with a collapsed coin for more egibbs-itude
  - two plots: somebody who either is or is not suspicious; the latter
    should penalize m-h and rejection; accuracy vs # transitions and
    accuracy vs runtime
- Desideratum: assessable compounds as proposals and transition
  operators (e.g., the type of transition operators, etc)

Vkm says:
- "All the inference algorithms from the Venture paper can be written
  as inference programs through the same abstractions (i.e., scaffold,
  regen, detach) and the implementations run against RandomDB and we
  believe they would also work with PETs."
  - vkm's vision is that detach/regen are parameterized by scaffolds
  - the natural scaffolds to build with RandomDB are ones that include
    the whole rest of the program, and absorb at (either all
    assessable choices or just observations)
  - vkm specifically wants there to be a thing that builds the
    scaffolds that are natural in a PET
    - candidate vocabulary: "maximal reconsideration scaffold" and
      "minimal reconsideration scaffold"
  - the impact that the scaffold has on the transition operator is
    through what is incorporated/unincorporated, through the proposal
    distribution that you get
  - also through spuriously resimulating likelihood-free procedures
*** Paper: Notes (early 2015) on Metaprob, after submitting to UAI (was rejected)
One concrete task: Introduce an indexing mechanism for \omega so that
one can make independent random variables by rng chaining.

pdf in email "metaprob draft notes"

----------------------------------------------------------------------

Confusion: the | in p(x|y,z) can mean "argument to generator", or can
mean "constraint".  For densities, there are no generators, so there
is no distinction.

Idea: have the basic computational unit be "constrained simulate",
which admits some language of constraints (unit-dependent), and emits
a value and a weight.  So, a unit representing
  p(X|Y,Z)
can emit weighted samples, such that the importanter represents (an
acceptable approximation to) the true distribution p(X|Y,Z); but
nothing is required of the distribution on the actual samples x.

The idea is that Z represents various possible constraints that might
be descendants of X.  Examples:
- Total equality would recover log density if the constraint were not
  treated as part of the distribution.  That is, log density is
  "sample me an x ~ p(X|Y), with proposal x = x_0, and report the
  weight under p(X|Y)."
- Can talk about partial equality/pattern matching constraints, and
  possibly about propagating parts of them back.
  - The crosscat givens are an instance of this?
- "Sample a tree with fringe Z from grammar Y"
***** Notes on improving the UAI submission, even for the arXiv
Goal: Something on the ArXiv whose content I am really pleased with.
Success condition: I don't have to say "There's this paper, but it has XYZ bug"

******* The actual paper is at https://cmt.research.microsoft.com/UAI2015/Protected/Author/ 
vkm@mit.edu
 ]Q:4RD.(

(note the leading space)
******* Reference: Architectural motivation for MetaProb, for Taylor's benefit
Probabilistic programs are representations of probability
distributions by the behavior of randomized machines.

It is deemed desirable to permit a division into "models" and
"inference on those models".  It is further deemed desirable to permit
definition of generic inference strategies that operate on any (or
many) models, so models need to be inspectable enough.  It is further
deemed desirable to permit models to be arbitrary probabilistic
programs themselves.

This leads to the following architecture:
- A model is defined by a probabilistic program.  The possibility
  space of the model is the set of possible execution histories of
  that program.
- To make use of this, any (sub)program may be run in a tracing
  version of the interpreter, which run produces (the value and) a
  trace object, that reifies that execution history.
- Different trace types impose different costs, and enable different
  inference strategies (sometimes at all; sometimes it's a matter of
  asymptotics); thus the tracing interpreter is pluggable.
- Procedures may be declaratively annotated with meta-information that
  various inference strategies may use if that procedure appears in a
  model.  For example, a procedure P may be annotated with an
  assessor, which is a procedure that knows how to compute the
  probability that P produces a given output from given inputs.
******* Random note: The one-state simulator phenomenon in the HMM is general to all data structures
A "strict" data structure is a sample from the joint distribution on
all the random variables (of ground type) that appear in the data
structure, so an assessor for such is necessarily an assessor of the
joint.  A "lazy" one is the collection of the random variables
themselves (which need not be independent), thus admits separate
simulators and assessors for each variable, marginalizing out the
other ones.
- But, careful! The procedure emitted by
  (let ((data (sample something strict)))
    (lambda (address)
      (lookup address data)))
  is not a sampler for the marginalized random variable, it's the
  conditional sampler for the component; its assessor must be
  conditional, and therefore be the uninteresting assessor of the
  above operation construed deterministically.
******* Notes from revising the UAI submission
I assume we want the arxiv thing to be useful as a milestone towards a
UAI camera-ready?
- Send the draft to Taylor (presumably after posting v1)
- vkm says: we can go a couple pages over space (there is budget for
  the fine)

Communication/decision:
- Check whether we delivered on the "core claim": vkm said "measure!
  measure space! what if jordan reads it? link density, measure, and
  simulator?"
- Check whether we still need to add whitespace on fig3
- vkm says "measure theory composable analysis"
- vkm says "apply for random variables"
- vkm says "panels for SLAM figure"
- vkm says "dynamic bayes net"
- vkm says "joint distribution for SLAM"
  - axch interprets: an expression for the joint density of the trace
    of SLAM at some time step, as a function of the values in it.
- vkm says "SLAM scopes? is there an error? should we show @tag?"
- vkm says "describe random motion model?"
- vkm says "real illustration of measures / indian GPA"
- vkm says "OPTIONAL: revise 1st paragraph; include BayesDB, Picture,
  Roofit?"
- design the interface for the shim for running literally the code of
  custom M-H, write the shim, and run literally that code.
  - let a trace be a map from symbols to values (and perhaps
    procedures with assessors or something) and writing macros for
    (trace-in ... (define ... ...))  and (trace-in ... ...)
- HMM asymptotic scaling comparisons [1]
- meta-inference or similar? [2]
- Why stress that traces can be mutable?  I was just thinking about
  refactoring the interpreter so that it will be agnostic to whether,
  e.g., trace-eval mutates the trace or returns a new one.
  - There is only one metion of this in the document now, in a
    paragraph heading.
- How soon do we want to mention that MetaProb also allows a program
  to be run untraced, and thus pay no tracing cost?
- normalize "inspect", "predict"
- Show vkm the "Architectural motivation for MetaProb, for Taylor's
  benefit"
- Neil Toronto says 'I would rather have read something like this:
  "All of these languages have baked-in inference algorithms. This is
  a problem because X. Metaprob solves this problem by Y."'
- Neil Toronto got to thinking of this as a system paper, and got
  confused by the system.
- Neil Toronto didn't like the dashed/non-dashed lines in the two
  figures that make the distinction.  Perhaps we can explicitly say in
  the caption what the lines mean.
- Neil Toronto says "The Discussion section is a bit talky and
  speculative. [Could be good or bad.]"
- Neil Toronto complained about lack of subfigure letters in figures 1
  and 2. (the latter may be moot now)
- Neil Toronto says "Emulating VentureScript is nice. Emulating other
  systems would be nicer."
- I am still quibbling with the claim that random variables and sample
  spaces are first-class.
- Is there a citation for Hansei/Hakaru?
- Is there a citation for Picture?

Thinking:
- What to say about random variables that don't halt, and where to say
  it?
- Specify a syntax for declarations; be explicit and clear about its
  utility; use it consistently.
  - something about being able to build automatic inference,
    controlling communication channel between meta-programs
  - vkm says "could rehash when discussing compatibility"
- Inconsistency: In Figure 2 it is suggested that eval mutates (by
  describing trace-in that way), but in Figure 5 it is suggested
  otherwise.
- "Sources of randomness" is not a term of art (abstract, figures)
  - The thing we show assumes that uniforms are the only primitives.
    This is fine, but maybe we should signpost that this is a choice,
    and other traces with other primitive sets are possible too.
    - Notably, unit uniforms require separation by index only, not by
      arguments.
    - Also, can we get the environment on the trace-in call to show up
      in the source of randomness block?
- A measure is allowed to delimit the set of events it deems to be
  measurable (subject to some conditions, like being closed under
  intersection, etc).  Perhaps we should be squishy about the measure
  interface, lest we appear to require a measure to be a thing that
  takes any computable predicate and produces a measurement, as that
  may be too strict.
  - In particular, I take the above philosophy to mean that it's
    appropriate for different measures to impose on their clients
    different representations of sets to be measured.  The most
    flexible one for the clients is of course predicates, but we
    should probably permit measures to be pickier.  And I really don't
    like doing that by source inspection on the predicate and erroring
    out at runtime if it is not acceptable.

Low risk but valuable:
- Mapping diagram:
  - Consistent typography; what's the rule?
  - Add arrows from the arguments to density and bound to the call
    expression, by analogy with lambda and apply in the top row?
  - Comment (in the figure? in the text?) that we don't expect every
    measure to be able to compute exact answers for all predicates in
    bounded time; but also that any simulator gives rise to a measure,
    whose judgements are even simple to approximate.
  - Give a math name to the density, by analogy to measure and bound?

Camera-ready nitpick:
- Indentation of code examples (lstlisting does this poorly)
- fix bug in bugs citation and cite bugs
- Make sure that the intro figure mentions that the distribution was
  generated from the particular data 4.62, 1.94, not marginalizing
  data sets as one might imagine.
- legend for the hmm figure: dashed lines are assessors, solid lines
  are calls
- Reorder panels to follow textual reference order: source of
  randomness, random variable, generative process.
  - A pain because the columns are not all the same width
  - Search text for references to it
- Do we want figure 7a to have legible ticks? (harder to get the source)

[1] Asymptotic scaling of HMM pieces
- allude to implicit representations of the matrices (because that's a
  place where space savings come from).
- particle filter doesn't pay space for the sequence length
- approx gibbs
  - pays only for distribution wierdness, not state space support
- forward filter backward sample exact sampler
  - pays for state space support

- Algorithmic complexity of discrete sampling, with hmm as the model program
  - varying screwiness of transition/observation functions; state
    space and sequence size
  - another model problem is sampling from a multivariate gaussian,
    varying e.g. condition number of the matrix

- vkm says:
  The point of HMM: things like forward-backward have a place in a world where
  distributions are samplers
  - most dynamic version does not degrade if params change all the time
  - static version: redo work if params change
    - forward filtering backward sampling, with prep
      - prep is forward filter, store e.g. z and the final state vector;
        then do backward sampling and assessing (which assessment is not
        so useful for fiddling with the parameters)

[2] Inference for debugging
- Adjustment: reuse the gamma-uniform-normal model
- Phenomenon to see in output samples is that it might or might not
  have rejected
- "The inference program being debugged should be specialized; perhaps
  the one doing the debugging can be specialized."
  - Style: specialized inference algorithm should use concepts from
    metaprob, like traces.
******* Reference: Rationale for Metaprob revision
vkm says: 

Short version: We don't have a standalone document yet that we can
hand to the key players (Mike Jordan; Suresh; Purush from ARO; Andrew
Gelman) and that they can digest and get anything out that resembles
what we would hope to have given them.  Which is to be expected; what
we have is a submittable UAI-format paper, which typically only has to
pass a lower bar


vkm says: questions the audience will have:

re ex1: from mike jordan: "how does all this relate to something I
know, e.g. a graphical model?" from anybody else: "what is this
inference trace and how does it show up in the code? how is this
different from church? and why is the plot so ugly and messy, what
point is that trying to show?"

re measures and densities: from any reader: "is there an example of
how measures and densities could work?", from mike jordan: "are
measures and densities just a hack? or is there something more general
at play here? where does all the hard measure theory i've learned fit
in to all this? how come that isn't front and center?"

re estimating Z: from mike or zoubin or geoff hinton: "how does the
partition function problem show up here? that's a key, perhaps the
key, of computation for probability. and it definitely would be needed
for these assessors. or would it? are they the joint distribution?
what about undirected models? those don't typically seem
generative. i'm confused."

re indian GPA: from mike jordan: "they can't possibly have
what they are claiming unless what they're claiming addresses my
example. have they thought it through? i certainly can't piece it
together from what they've written"
******* Reference: "homework notes" given by vkm 3/18/15:
- make ex1 better:
  - split the plots into separate seaborn
    contours (maybe scatters); ask bax about this.
  - add syntax for explicitly controlling inference tracing, so the
    code shows that the inference program is, in fact, actually
    traced. oops :)      
  - optional: draw a graphical model for this example, showing the
    markov chain over the random choices for each iteration, consuming
    proposals, and making accept reject decisions; then overlay/shade these
    so that it is clear which trace each choice falls in


- give default approximation techniques (asymptotically exact) for
  evaluating measures in predicate form (Monte Carlo) and for
  estimating densities (kernel density estimation, or a DP mixture
  model), and briefly discuss the costs/benefits of this view.

  possibly also describe exact enumeration for procedures that have
  a finite space of trace fragments, possibly even referencing
  probscheme :)

- relate estimating measures to estimating Z (assuming an
  unnormalized assessor). for simulators with discrete outputs (that
  do not come with any annotations suggesting special additional
  structure), this is just the measure of (lambda (x) True). for
  simulators with continuous outputs, an assessor is needed.

  It might be interesting to talk about how knowledge about the
  assessor translates to estimating Z, e.g. "the assessor is
  integrable", "the assessor has known bound x, and is therefore
  integrable by strategy y", and relate this to halting.

- re Indian GPA: I think I agree with your summary and have several
  questions about the internals. I think the important things to
  show are:

  - programs (and plots, with impulses) for non-noisy and noisy
    versions of the problem

  - the special case where it is packaged as an SP

  - a schematic showing the static/dynamic analysis problem that
    needs to be solved for a generic algorithm to "do the right
    thing"

    - and give a simple, clear, conservative answer for what
      "we actually do", possibly grounded in the implementation of MH
      via regen/detach that we actually show

  - other notes: i don't think we have any obligations wrt propagation
    of this kind of analysis. i think it is the right thing for the
    SP-encapsulated form to be unable to move the spike (but, say, be
    able to do inference on the mixing weight); this should of course
    change when there is noise.
*** Paper candidate: Exchangeability, for an ICFP-type audience
Vikash got excited by the exchangeability constraint on mutation being
a way to recover having syntactic independencies, as something
publishable for ICFP.  The story I told that excited him was:
- A pure probabilistic program has syntactic independence guarantees:
  repeated applications of a procedure are exchangeable.
  - iid conditioned on the contents of the closure.
- If mutation is added, those guarantees go away (but, depending on
  the actual mutations, various random variables may nonetheless be
  independent and/or exchangeable).
- Can always flush the mutation by doing a global transform to
  store-threading style.
  - However, back-translating the guarantees that are syntactically
    apparent in that representation produces not very useful
    guarantees in the original source: the future depends on the past.
  - A finer-grained study of the store (cf alias analysis) would give
    finer dependencies, but the future applications of a given thunk
    would remain dependent on the past ones.
- But, if the mutation obeys the appropriate constraints (Abelian
  group update), we still have all the syntactic guarantees we had in
  the pure case.
  - Hm.  Does this require any contraints on how the mutating object
    is read?  If so, what are they?

He mentioned it 9/28/15 as something he regrets not having had as much
time as he wanted for

On 11/30/15, I decided to archive it.
*** User-visible change-log, as of 4/14/15
- User-facing changes that might be announced?
  - Made these announcements to Baxter and Taylor; anyone else?
    - Announce the explicit quasiquote macro and the ` reader macro.
    - Announce that developers can rebuild the reference manual now(?)
    - Announce elimination of 'mixture' and 'cycle' inference syntax;
      with replacements.
      - Some of the tutorial IPython notebooks are now (more) broken.
    - Announce the reference manual
      - Also, inference prelude functions and call backs now documented
      - Milestone met: every referencable symbol now occurs in the
        generated documentation somewhere.
  - I haven't made these announcements:
    - Announce freeze and forget in the inference language
    - Announce optional labels in assume, observe, predict inference SPs
    - Announce default labeling of assumes
    - Announce new_model and in_model
    - Announce cond and letrec, thanks to Anthony
  - More, reconstructed from refman-changelog (since Mar 9 17:37,
    commit 022429a086d367e221f2c2d7f0ea4bf75527aca9):
    - Add mod, atan2, vonmises to Lite
    - Add a gradient for second to Lite
    - Rename scope_include, scope_exclude to tag, tag_exclude (in Lite and Puma)
    - Make the noise argument of "exactly" optional (in Lite)
    - Add an optional attempt bound to rejection (in Lite)
    - Add vector_dot to Puma
    - Add new_model, in_model
    - Delete the ancient cxx backend
    - Infer loop now takes one action rather than a list
*** Notes on plotting features
We theoretically want compositing across particles and animation by sweep count
- [Taylor?] Figure out how to composite; maybe do all of this?
  1) Choose a client interface of representations of images you will accept
     - It would be nice if there were a simple incantation that takes a
       matplotlib figure and emits an image of the chosen form, but should
       not be restricted to matplotlib
  2) Write a function that takes several images in that representation and
     returns one, alpha blended
     - Variants that save to disk in some standard format (png?) or that
       immediately display on screen
  2b) Allow the client of the above to weight the images, affecting their
     importance in the blend
  3) Another function, that takes a list or generator (your choice) of
     images and produces a movie
     - Variants that save the movie to disk, or pre-compute and display
       on screen in batch (external viewer is fine), or [optional]
       display streaming
  - Resources: pygame (library for writing video games in Python),
    pil, pycairo
- In foreign callbacks:
  - Maybe offer compositing as a combinator that wraps a function with map
    - The function is to return an image (png?)
    - Libraries to consider: pil, pycairo
  - Leave animation to the client
- Integrated with plotf:
  - Just add compositing as a fourth dimension (meaningful options
    for it include particle id and particle weight; should hack
    particle ids not to over-darken particles with large ids)
  - Separate into plotf_now and plotf_accumulate
  - "Offline video a": Tweak plotf_now_to_file to include the sweep
    counter in the file name (obeying formatting directives?), which
    can then be composed into a movie offline.
    - Usage idiom: delete all the frames && venture foo && make the movie && display it
  - "Offline video b": plotf_accumulate can use matplotlib animation?
  - "Streaming video": Make a variant of plotf_now that doesn't wait
    for you to dismiss the window, but writes over it every time
  - True streaming video would be that by piping to a viewer that
    remembers and lets you go back in time (key frame interpolation!?)
- Deliver test example(s) (for plotf abstract and for others to use
  while in hectic states)
- Another possible dimension can be mapped to faceting
- Can we have additive modifiers for how to treat overplotting?
  (Exactly identical discrete samples; continuous samples that are
  close enough for the points to overlap).  Options: control or good
  choice of point size?; 2D kernel density nonsense?
*** Notes on error recovery (e.g. at the console)
When a mistake happens in a live session, what may be wrong?
- The (persistent?) inference trace may be partially detached or
  regenerated, making it unusable.
- The model trace ditto
  - e.g., an error during assume will leave a garbage predict of
    everything it managed to assume before the error
- Some model held in some variable ditto
  - Though, may not be accessible, since the console (currently) does
    not maintain any explicit inference variables that can be altered
    by actions.
- Some plugin's internal state may be in an inconsistent state
- Who knows what other invariant may be violated?  But the above are
  the most common in present experience.
What actions are reasonable in these cases?
- Forget the relevant section of the inference trace and continue
  (but can I actually do that?)
- Rebuild the inference trace and continue
  - will lose outstanding defines
  - can probably be recovered by loading a file
- Rebuild the inference trace replaying defines
  - need to write that code (definition memory wrapper, etc)
  - Currently "define"s can't actually take any actions, so replaying
    them is safe (may repeat prints, perhaps).
- Rebuild the model and continue
  - will lose outstanding assumes, observes, predicts
  - can probably be recovered by loading a file
- Rebuild the model from the prior and continue
  - one of the ways to implement reinit_inference_problem would do that
  - will lose current (potentially interesting) state, but presumably
    the model had a bug anyway
- "Continue" from model errors may mean editing the model
  - Could blow it away and reload, losing state
  - Could try to make "reassume" work (uneval, eval, propagate the
    change downstream)
- "clear" and "load" should more or less fix anything.
  + could add "reload"
  + could also define "clear" in the console to just stop continuous
    inference and construct a new RIPL (perhaps replaying the
    commandline arguments?)
*** Research: Replicate BLOG in Venture (or come up with another open universe story)
Skeletal plan for replicating BLOG in Venture:
- First, figure out what inference algorithm(s) the actual BLOG
  actually has, 'cause if it's just likelihood weighting, the
  inference quality replication task is trivial (and computational
  efficiency replication is out of the question anyway).
- Second, if there is a need to make M-H chains over sets work, do the
  scaffold visualization project (highlighting model source code).
- Third, do again what David tried to do on the blog-examples branch,
  namely
  - Brainstorm possible representations of sets, set-element-attribute
    associations, etc.
  - See (using the scaffold visualizer) what kinds of proposals can be
    recovered for them under what circumstances; use it to debug
    library implementations.
- Alternately or afterward, think about foreign SPs for set-level
  manipulations.

Reference:
- Email "Report coming your way soon-ish" and the PDF therein
- The blog-examples branch of Venturecxx
***** Reference: Questions I had for David, before the latest round of attempts
- How well does he understand the dynamics of this?  How good are
  they?  The real question is: if I think something is weird, am I
  probably right, or am I probably missing something?
  - e.g. token/event sets as strict lists rather than thunk lists or
    mem-tables: proposing to one will likely rebuild at least the
    whole spine, and possibly also repropose all the earlier ones
    (depending on details of what detach ends up doing).
  - e.g. token_attrs token_location will be a block including
    all the locations of all the tokens; is that a good idea?
  - similar concern as above with the true_detections set
  - Why is detected_sign not memmed?  Ditto mean_time_true,
    variance_time_true, detected_time_*?
    - May be ok if they are only called once, inside a memmed thing.
      - Which is not true of detected_time_true, if the event map is
        not 1-1
      - Also not true of variance_time_true, which will matter if its
        body stops being a constant.
  - What's with that 29?
  - Might want to raise the mh transition counts: they are relatively
    cheap, compared to running the inference program (especially if
    this runs in Puma).
  - Why is it useful to have a thing that has a logDensity but never
    absorbs?  I would think that would cause the logDensity to never
    be called.
- cycle in [define runner ... ] in seismic_test.vnt has been deprecated
  - ggplot may still work on my machine; perhaps resurrect?
  - How long did those tests take to run?
- plot_demo_1 and plot_demo_2 are supposed to work; what resources do
  they consume?

- Meta-point: measuring and visualizing the scaffolds that various
  proposals lead to would make it much easier to reason about choices
  such as strict lists vs thunk lists.
  - Is this about cost-center-like profiling of frequency of appearing
    in scaffolds?  Maybe broken down by principal nodes?

*** Candidate semantic addition: Backward model extension
There is another operation that one can do in light of a generative
model that regen is also good for.  The example is the backward step
in the backward part of the forward-backward algorithm.  To wit, in
the presence of a materialized value for some variable Z, extend the
currently materialized portion of the model to include a value x for
some X, and compute the weight corresponding to p(Z=z|X=x).  This is
dual to extending a model that has a materialized x with a fixed value
of z (i.e., regeneration with a "DeterministicLKernel").
*** Candidate semantic addition: Non-transient tori
Non-transient tori permit another useful idea: extending a model with
an expression without evaluating (regenerating) the expression.  If
this operation is implemented, it becomes possible to put in the
constraint of an observation first and then regenerate with the
constraint already in place, thereby not calling the simulator (and
not needing the simulator to exist, etc).
*** Reference: system architectural thoughts circa early March 2015
Interesting features the current Venture architecture has
- The ability to report something about the current model distribution
  (either it's the definition of the concrete distribution, which
  involves recording the whole program, or it's meant to be the
  definition of the current sample space and ideal posterior, which
  would just be non-forgotten assumes, observes, predicts (albeit
  buggy, because there is no way to record the result of a freeze)
  (and is itself a random variable, because the inference program that
  gave rise to it may have made random choices)).
- Addressing schemes by which aspects of the current model
  distribution may be inspected or modified
  - scopes and blocks for targeting inference
  - labels or directive ids for targeting interventions like freeze
    and forget
    - I guess the distinction is that only toplevel things may
      currently be targeted by such interventions?
    - Forget only makes sense for toplevel things anyway
    - Freeze gets really confusing unless you hit all dynamic
      occurrences of the same static code (otherwise there is no
      longer a program whose execution history the current trace is).
  - internal addresses, meant for profiling and error reporting (are
    they also used in serialization?)
  - another bug: not every runtime address will be valid in every
    particle.  How are we dealing with that?
- Parsing, with a notion of parse error locations.
- Unparsing (I think this is broken for programmatically generated
  expressions, namely quasiquote)
- Error reporting (sometimes).
- Some moderate quantity of error-checking in the core sivm.
- Implementer-defined macros.
- An interpreter that runs the inference program in one trace and the
  model program in another, nearly capable of managing multiple model
  programs.
- Parallel operations across multiple traces.
- Inner loop evaluation available in a Python or a C++ backend.

Where these features should live:
- There should be a clean interface to inner loop evaluation, so the
  Python and C++ backends can both implement it.
  - Maintaining a trace
  - Some compounding of inference loops, as is currently done
  - Ideally, error reporting with locations
- Immediately around one trace, ability to report and mutate the
  current sample space and ideal posterior.
  - Source recording, "directive ids", etc.
  - Arbitrary Eq-able VentureValues as labels?
  - Track the incorporation state of constraints, because those affect
    the "ideal" posterior (and can source bugs).
  - Doesn't need to be cloned across backends.
  - Freeze means this may vary across particles.
  - Lite's trace copying (and serialization?) strategy depends on
    knowing the program structure.
  - This means a trace can locally interpret error locations,
    construct stack traces, etc.
- Around that, a (possibly parallelizing) TraceSet for particle methods
  - Probably separated into the actual parallel handling and the
    set-level operations like resampling.
- Around that, an interpreter for the inference programming language,
  with its own error reporting; be able to manage TraceSets as values.
- Parsing, type checking (if any), and macro expansion should be
  callable (iso)morphisms (e.g., so that eval may call them).
- Something needs to have a sufficiently friendly API that will parse
  strings, interpret errors, and possibly implement a "best effort"
  isomorphism between Python and Venture values.

Why Python is a pain in the arse for this layer:
- I really want strong (whether static or dynamic) typing here, so the
  contracts and data representations can be very clear.
- Python exceptions are difficult to treat as manipulable objects.
  - The ideal error report for a crash in a foreign SP would give the
    inference program trace, the model program trace, and then the
    trace inside the foreign SP.
- Parallel computing in Python is pretty hopeless.
- There are no good Python parser generators.

----------------------------------------------------------------------

Let's write the above again in terms of services, places it makes
sense to provide them, and why.

Per-backend Trace (including backend/new_cxx/trace.py)
- RNG state (in Puma)
- Node graph of an individual execution history (trace.families)
  - regen/detach r/w
  - toplevel eval adds, uneval removes
- Scopes map thereof (incl. trace.rcs)
  - regen/detach writes, scaffold reads
- Constrained choice set
  - regen/detach affect this via constrain/unconstrain
- Unpropagated observations
  - observe adds, incorporate removes
  - How does this interact with the unconstraining that regen does if
    control flow changes?
- Global environment
  - passed to regen explicitly, adjusted by toplevel assume, bind_foreign_sp
  - mapping between ids and top nodes
- AEKernel storage
  - ??
- Also a bunch of indirections for storing things that could live on
  Nodes, for the persistent particle use case.
- Regen/Detach
- Scaffold construction
- Primitive inference operator implementation
  - The iteration loops want to be migratable to C++
- Profiling information
  - Collected by instrumenting primitive inference operators; wants to
    live somewhere persistent.

Possible refinements:
- Stuff regen/detach interact with
- Stuff that needs to be different for persistent particles
- Stuff scaffold construction interacts with
- Layer for unpropagated observations
- Layer? separate object? for profiling information
- Layer for recording source expressions

engine.trace.Trace
- implements define, evaluate, observe, and such for one trace in
  terms of eval, bindInGlobalEnv
- directive cache
- serialization/deserialization of traces
- reinit_inference_problem which resamples the model from the prior
  based on the stored directives

multiprocess.Worker
- messes with IPC on the slave side
- catches exceptions quasi-serializably

multiprocess.Master
- messes with IPC on the master side
- chunking and mapping over multiple underlying objects
- reconstitution and rethrowing of exceptions serialized by workers

TraceSet
- Point of selection for Puma vs Lite backend (by accepting the constructor)
- Multi-particle modeling
  - choice of whether operations are routed to the model "map"ped or
    evaluated at one point
  - log_weights; operations that impact them
  - resampling and changes of across-trace parallelism style
  - handles changes in the number of model traces from other inference
    SPs (diversify, collapse)
  - including retrieve_dump{s}, retrieve_trace{s} for serialization
    and resampling

Engine
- Run-time backend swapping API
- Management of an inference trace as distinct from a TraceSet of
  model traces
  - assume, observe, predict, report_value get routed to the model
  - define, infer get routed to the inference trace
- Inference prelude by somewhat grotty hackery
- Registry of bound foreign sps and bound foreign inference sps
- Assigns globally unique directive ids
- Actual thread management for continuous inference, inlcuding infer loop
  - delegated to ContinuousInferrer, but still
- Inference callback table
- Records time since creation, presumably for plotf to read
- ensure_rng_seeded_decently (?)
- get_entropy_info (?)
- some respectable interaction with whole-system serialization
- Theoretically supposed to enable RNG state management, but
  practically doesn't

Infer
- Exposed as the object directly manipulated by inference SPs
- parsing collect's kooky sublanguage of what to do with each expression
- Actually implement collect, plotf, printf
- Delegate to the engine (sometimes with minor tweaks) for all other
  inference action SPs (except the ones that bypass it and go to the ripl)
- A few convenience methods for callbacks (e.g. particle_normalized_probs)

CoreSivm
- Interprets data instructions as engine method calls
  - Some (runtime!) type validation of the instructions
  - Munges the incoming and outgoing data some, with the _modify_foo
    functions.
- Reports the values of observe instructions (because the engine
  allegedly doesn't)
- Introspection on whether the profiler is enabled
- Some interaction with global serialization (namely storing the
  observe_dict)

VentureSivm
- Performs macro expansion on incoming model and inference programs
  - Stores a table mapping produced directives to pre-expansion
    expressions for error annotation (this is very brittle!)
- Annotates exceptions from instruction execution with error
  locations, in terms of pre-macro-expansion expressions.
- Pauses continuous inference (in terms, ultimately, of the Engine's
  methods) when other instructions are invoked.
- Maintains labels of "labeled" instructions, and resolves them
  when some other instruction refers by label.
- Implements list_directives (by storing the directives separately
  from the Engine)
- Implements several "macro" instructions, like force and sample (and
  then the Engine implements them again, to give inference SPs and
  others access)
  - This includes interpreting the data structure to decide what to do

Ripl
- Parses string-form (and partially structured) instructions into
  structured form (by delegating to a parser)
- Parses string-form programs into sturctured instruction streams
- Provides an API frontend to the data instructions (as an alternative
  to string parsing)
- Annotates errors from lower layers with string-form location
  indicators
- Provides a multi-instruction program execution loop (much like do or
  begin, lower down).
- Loads the model prelude, if requested
- Loads the default plugin
- bulk_observe and observe_dataset, such as they are
- Some delegation directly to the underlying engine, without defining
  instruction data representations
- Some machinery for data display for Vlad's actual profiler
- General plugin loading

Console
- An actual terminal read-eval-print loop
- Delegates to ripl with minor tweaks

A bug: the object that represents a complete, encapsulated system
state that may be interacted with by the external API is currently
conflated with the object that accepts and produces the string-based
(thus presumably human-friendly) language encoding.
- This is a bug because it muddles inference programs programmatically
  controlling models

Some architectural rationales:
- engine.trace.Trace and down needs to be fully serializable, so
  cannot maintain the foreign sp registry.
- Everything at TraceSet down should be agnostic as to whether it is
  used for the "model" or the "inference program"
- Master/Worker should be as agnostic as possible about the objects
  they are managing, because just general IPC is hard enough.

*** Reference: Issues with the LKernel interface
- Issue: The current LKernel interface forbids LKernels that can
  compute their own densities and do not pretend to be able to account
  for the prior (e.g., DeterministicLKernel, and Gaussian drift
  applied to a non-Gaussian prior SP), except by hackery.  Perhaps I
  should enable that.
- Issue: The current LKernel interface also doesn't support symmetric
  kernels very well, in that they become required to compute the prior
  ratio instead of being able to just cancel their own density against
  themselves and go home.
- Does Puma not have AAA for uncollapsed models?  The Gibbs steps are
  implemented as AEKernels; does that mechanism have the desired
  effect of suppressing traversal of the children when proposing?
- Option: change the method name(s) for AAALKernels, to strengthen the
  indication that their obligations differ.
- Yes, there is conceptually such a thing as a DeltaAAALKernel:
  Gaussian drift applied to the weight argument of
  make_suff_stat_bernoulli.
- Does Puma need to be tweaked to make DeterministicMakerAAALKernel
  not the default, but explicitly named by all the examples?  In Lite
  this is another subclass.
